{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e28767",
   "metadata": {},
   "source": [
    "# Web Scraping Fundamentals: From HTML to Data ğŸŒ\n",
    "\n",
    "## What is Web Scraping? ğŸ¤”\n",
    "\n",
    "**Web scraping** is the process of automatically extracting data from websites. Think of it as teaching your computer to \"read\" web pages like a human would, but much faster and more systematically.\n",
    "\n",
    "**Imagine this scenario:** You want to collect product prices from an e-commerce site, track news headlines, or gather research data from multiple websites. Instead of manually copying and pasting for hours, web scraping automates this process!\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **ğŸ” The anatomy of web pages** - HTML structure and how data is organized\n",
    "2. **ğŸ› ï¸ Essential scraping tools** - requests, BeautifulSoup, and modern alternatives\n",
    "3. **ğŸ“Š Data extraction techniques** - Finding and extracting specific information\n",
    "4. **âš–ï¸ Legal and ethical considerations** - Scraping responsibly and respectfully\n",
    "5. **ğŸš€ Advanced concepts** - Handling dynamic content, APIs, and challenges\n",
    "6. **ğŸ’¼ Real-world applications** - Practical examples and use cases\n",
    "\n",
    "## ğŸŒŸ Why Web Scraping Matters\n",
    "\n",
    "**In Data Science:**\n",
    "- **Data Collection:** Gather datasets for analysis and machine learning\n",
    "- **Market Research:** Monitor competitors, prices, and trends\n",
    "- **Automation:** Replace manual data entry with automated processes\n",
    "\n",
    "**In Business:**\n",
    "- **Lead Generation:** Extract contact information and business data\n",
    "- **Content Aggregation:** Collect news, reviews, and social media posts\n",
    "- **Monitoring:** Track mentions, reviews, and brand sentiment\n",
    "\n",
    "**The Big Picture:** Web scraping democratizes data access, turning the entire web into your database! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde20a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Web Scraping Toolkit Ready!\n",
      "ğŸ“š Libraries loaded:\n",
      "  â€¢ requests: HTTP requests and web communication\n",
      "  â€¢ BeautifulSoup: HTML parsing and navigation\n",
      "  â€¢ pandas: Data manipulation and analysis\n",
      "  â€¢ json: Handle JSON data from APIs\n",
      "  â€¢ urllib: URL handling and robots.txt checking\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization and data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"ğŸ”§ Web Scraping Toolkit Ready!\")\n",
    "print(\"ğŸ“š Libraries loaded:\")\n",
    "print(\"  â€¢ requests: HTTP requests and web communication\")\n",
    "print(\"  â€¢ BeautifulSoup: HTML parsing and navigation\")\n",
    "print(\"  â€¢ pandas: Data manipulation and analysis\")\n",
    "print(\"  â€¢ json: Handle JSON data from APIs\")\n",
    "print(\"  â€¢ urllib: URL handling and robots.txt checking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912eb68",
   "metadata": {},
   "source": [
    "## Chapter 1: Understanding HTML Structure ğŸ“\n",
    "\n",
    "**Before we scrape, we need to understand what we're scraping!** HTML (HyperText Markup Language) is the backbone of web pages.\n",
    "\n",
    "### ğŸ—ï¸ HTML Anatomy\n",
    "\n",
    "**Think of HTML like a document outline:**\n",
    "- **Tags:** Define structure (headings, paragraphs, lists)\n",
    "- **Attributes:** Provide additional information (id, class, href)\n",
    "- **Content:** The actual text and data we want to extract\n",
    "\n",
    "**Common HTML Elements:**\n",
    "```html\n",
    "<html>                    <!-- Root element -->\n",
    "  <head>                  <!-- Metadata -->\n",
    "    <title>Page Title</title>\n",
    "  </head>\n",
    "  <body>                  <!-- Visible content -->\n",
    "    <h1 id=\"main-title\">Heading</h1>\n",
    "    <p class=\"description\">Paragraph text</p>\n",
    "    <div class=\"container\">\n",
    "      <ul>                <!-- Unordered list -->\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "    <a href=\"https://example.com\">Link</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### ğŸ¯ CSS Selectors: Your Navigation Tool\n",
    "\n",
    "**CSS selectors help us find specific elements:**\n",
    "- **Element:** `p` (all paragraphs)\n",
    "- **Class:** `.description` (elements with class=\"description\")\n",
    "- **ID:** `#main-title` (element with id=\"main-title\")\n",
    "- **Descendant:** `div p` (paragraphs inside divs)\n",
    "- **Attribute:** `a[href]` (links with href attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6631f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ MAKING HTTP REQUESTS\n",
      "==============================\n",
      "ğŸ“„ HTML PARSING DEMONSTRATION\n",
      "===================================\n",
      "Product Title: Amazing Laptop\n",
      "Price: $999.99\n",
      "Description: High-performance laptop with 16GB RAM\n",
      "Features: ['16GB RAM', '512GB SSD', 'Intel i7 Processor']\n",
      "\n",
      "Reviews:\n",
      "  1. 5 stars - Excellent laptop!\n",
      "  2. 4 stars - Great value for money.\n",
      "\n",
      "ğŸ¯ Key BeautifulSoup Methods:\n",
      "  â€¢ find() - Gets the first matching element\n",
      "  â€¢ find_all() - Gets all matching elements\n",
      "  â€¢ .text - Extracts text content\n",
      "  â€¢ .get() - Gets attribute values\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Making your first HTTP request\n",
    "print(\"ğŸŒ MAKING HTTP REQUESTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Example HTML content (simulating a simple webpage)\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Product Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 id=\"product-title\">Amazing Laptop</h1>\n",
    "        <p class=\"price\">$999.99</p>\n",
    "        <p class=\"description\">High-performance laptop with 16GB RAM</p>\n",
    "        <ul class=\"features\">\n",
    "            <li>16GB RAM</li>\n",
    "            <li>512GB SSD</li>\n",
    "            <li>Intel i7 Processor</li>\n",
    "        </ul>\n",
    "        <div class=\"reviews\">\n",
    "            <div class=\"review\">\n",
    "                <span class=\"rating\">5 stars</span>\n",
    "                <p class=\"comment\">Excellent laptop!</p>\n",
    "            </div>\n",
    "            <div class=\"review\">\n",
    "                <span class=\"rating\">4 stars</span>\n",
    "                <p class=\"comment\">Great value for money.</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"ğŸ“„ HTML PARSING DEMONSTRATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Extract different types of information\n",
    "product_title = soup.find('h1', id='product-title').text\n",
    "price = soup.find('p', class_='price').text\n",
    "description = soup.find('p', class_='description').text\n",
    "\n",
    "print(f\"Product Title: {product_title}\")\n",
    "print(f\"Price: {price}\")\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "# Extract multiple elements (features list)\n",
    "features = soup.find_all('li')\n",
    "feature_list = [feature.text for feature in features]\n",
    "print(f\"Features: {feature_list}\")\n",
    "\n",
    "# Extract reviews\n",
    "reviews = soup.find_all('div', class_='review')\n",
    "review_data = []\n",
    "for review in reviews:\n",
    "    rating = review.find('span', class_='rating').text\n",
    "    comment = review.find('p', class_='comment').text\n",
    "    review_data.append({'rating': rating, 'comment': comment})\n",
    "\n",
    "print(f\"\\nReviews:\")\n",
    "for i, review in enumerate(review_data, 1):\n",
    "    print(f\"  {i}. {review['rating']} - {review['comment']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Key BeautifulSoup Methods:\")\n",
    "print(f\"  â€¢ find() - Gets the first matching element\")\n",
    "print(f\"  â€¢ find_all() - Gets all matching elements\")\n",
    "print(f\"  â€¢ .text - Extracts text content\")\n",
    "print(f\"  â€¢ .get() - Gets attribute values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715eefa",
   "metadata": {},
   "source": [
    "## Chapter 2: Real-World Web Scraping ğŸŒ\n",
    "\n",
    "**Now let's scrape actual websites!** We'll start with a simple, scraper-friendly site and demonstrate best practices.\n",
    "\n",
    "### ğŸ›¡ï¸ Best Practices for Responsible Scraping\n",
    "\n",
    "**Before scraping any website:**\n",
    "1. **Check robots.txt** - See what's allowed: `website.com/robots.txt`\n",
    "2. **Use proper headers** - Identify yourself as a legitimate user\n",
    "3. **Respect rate limits** - Don't overwhelm servers with requests\n",
    "4. **Handle errors gracefully** - Networks fail, pages change\n",
    "5. **Consider APIs first** - Many sites offer data through APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd60d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PROFESSIONAL WEB SCRAPING SETUP\n",
      "========================================\n",
      "\n",
      "ğŸ“š EXAMPLE: SCRAPING QUOTES FROM QUOTES.TOSCRAPE.COM\n",
      "=======================================================\n",
      "ğŸ¤– Checking robots.txt...\n",
      "  Can fetch http://quotes.toscrape.com/page/1/: True\n",
      "  Attempting request to: http://quotes.toscrape.com/page/1/...\n",
      "  âœ… Success! Status: 200\n",
      "\n",
      "ğŸ“– Found 10 quotes:\n",
      "----------------------------------------\n",
      "1. \"â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing...\" - Albert Einstein\n",
      "   Tags: change, deep-thoughts, thinking, world\n",
      "\n",
      "2. \"â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€...\" - J.K. Rowling\n",
      "   Tags: abilities, choices\n",
      "\n",
      "3. \"â€œThere are only two ways to live your life. One is as though nothing is a miracle. The other is as t...\" - Albert Einstein\n",
      "   Tags: inspirational, life, live, miracle, miracles\n",
      "\n",
      "ğŸ“Š SCRAPED DATA SUMMARY:\n",
      "  â€¢ Total quotes: 10\n",
      "  â€¢ Unique authors: 8\n",
      "  â€¢ Most common tags: {'inspirational': 3, 'life': 2, 'humor': 2}\n",
      "\n",
      "ğŸ“‹ Sample DataFrame:\n",
      "                                               quote           author  \\\n",
      "0  â€œThe world as we have created it is a process ...  Albert Einstein   \n",
      "1  â€œIt is our choices, Harry, that show what we t...     J.K. Rowling   \n",
      "2  â€œThere are only two ways to live your life. On...  Albert Einstein   \n",
      "3  â€œThe person, be it gentleman or lady, who has ...      Jane Austen   \n",
      "4  â€œImperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
      "\n",
      "                                       tags_str  \n",
      "0        change, deep-thoughts, thinking, world  \n",
      "1                            abilities, choices  \n",
      "2  inspirational, life, live, miracle, miracles  \n",
      "3              aliteracy, books, classic, humor  \n",
      "4                    be-yourself, inspirational  \n"
     ]
    }
   ],
   "source": [
    "# Example 2: Professional web scraping with proper headers and error handling\n",
    "print(\"ğŸ”§ PROFESSIONAL WEB SCRAPING SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a properly configured requests session\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Set headers to appear like a real browser\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    })\n",
    "    \n",
    "    return session\n",
    "\n",
    "def safe_request(url, session, timeout=10, retries=3):\n",
    "    \"\"\"Make a safe HTTP request with retries and error handling\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"  Attempting request to: {url[:50]}...\")\n",
    "            response = session.get(url, timeout=timeout)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            print(f\"  âœ… Success! Status: {response.status_code}\")\n",
    "            return response\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  â° Timeout on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  ğŸ”Œ Connection error on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"  ğŸš« HTTP error: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Unexpected error: {e}\")\n",
    "        \n",
    "        if attempt < retries - 1:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"  â³ Waiting {wait_time} seconds before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"  âŒ Failed to fetch {url} after {retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def check_robots_txt(base_url):\n",
    "    \"\"\"Check robots.txt for scraping permissions\"\"\"\n",
    "    try:\n",
    "        robots_url = urllib.parse.urljoin(base_url, '/robots.txt')\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except:\n",
    "        print(f\"  âš ï¸ Could not fetch robots.txt for {base_url}\")\n",
    "        return None\n",
    "\n",
    "# Example: Scraping quotes from a test website\n",
    "print(\"\\nğŸ“š EXAMPLE: SCRAPING QUOTES FROM QUOTES.TOSCRAPE.COM\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# This is a website specifically designed for scraping practice\n",
    "base_url = \"http://quotes.toscrape.com\"\n",
    "url = f\"{base_url}/page/1/\"\n",
    "\n",
    "# Check robots.txt\n",
    "print(\"ğŸ¤– Checking robots.txt...\")\n",
    "robots = check_robots_txt(base_url)\n",
    "if robots:\n",
    "    user_agent = '*'\n",
    "    can_fetch = robots.can_fetch(user_agent, url)\n",
    "    print(f\"  Can fetch {url}: {can_fetch}\")\n",
    "\n",
    "# Create session and make request\n",
    "session = create_session()\n",
    "response = safe_request(url, session)\n",
    "\n",
    "if response:\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract quotes data\n",
    "    quotes_data = []\n",
    "    quote_elements = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    print(f\"\\nğŸ“– Found {len(quote_elements)} quotes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, quote_elem in enumerate(quote_elements, 1):\n",
    "        # Extract quote text\n",
    "        quote_text = quote_elem.find('span', class_='text').text\n",
    "        \n",
    "        # Extract author\n",
    "        author = quote_elem.find('small', class_='author').text\n",
    "        \n",
    "        # Extract tags\n",
    "        tag_elements = quote_elem.find_all('a', class_='tag')\n",
    "        tags = [tag.text for tag in tag_elements]\n",
    "        \n",
    "        quote_data = {\n",
    "            'quote': quote_text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        }\n",
    "        quotes_data.append(quote_data)\n",
    "        \n",
    "        # Display first few quotes\n",
    "        if i <= 3:\n",
    "            print(f\"{i}. \\\"{quote_text[:100]}...\\\" - {author}\")\n",
    "            print(f\"   Tags: {', '.join(tags)}\")\n",
    "            print()\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    quotes_df = pd.DataFrame(quotes_data)\n",
    "    print(f\"ğŸ“Š SCRAPED DATA SUMMARY:\")\n",
    "    print(f\"  â€¢ Total quotes: {len(quotes_df)}\")\n",
    "    print(f\"  â€¢ Unique authors: {quotes_df['author'].nunique()}\")\n",
    "    print(f\"  â€¢ Most common tags: {pd.Series([tag for tags in quotes_df['tags'] for tag in tags]).value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(f\"\\nğŸ“‹ Sample DataFrame:\")\n",
    "    quotes_df['tags_str'] = quotes_df['tags'].apply(lambda x: ', '.join(x))\n",
    "    display_df = quotes_df[['quote', 'author', 'tags_str']].copy()\n",
    "    display_df['quote'] = display_df['quote'].str[:50] + '...'\n",
    "    print(display_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Could not fetch the webpage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2425b5",
   "metadata": {},
   "source": [
    "## Chapter 3: Advanced Scraping Techniques ğŸš€\n",
    "\n",
    "**Beyond basic scraping:** Real websites often require more sophisticated approaches.\n",
    "\n",
    "### ğŸ”„ Handling Dynamic Content\n",
    "\n",
    "**Many modern websites load content with JavaScript.** For these sites, you might need:\n",
    "\n",
    "1. **Selenium WebDriver** - Controls a real browser\n",
    "2. **Playwright** - Modern browser automation\n",
    "3. **requests-html** - JavaScript support for requests\n",
    "\n",
    "### ğŸ“‹ Forms and Sessions\n",
    "\n",
    "**Some data requires:**\n",
    "- **Login authentication** - Maintaining sessions\n",
    "- **Form submissions** - POST requests with CSRF tokens\n",
    "- **Pagination** - Following \"next page\" links\n",
    "- **AJAX requests** - API calls that load data dynamically\n",
    "\n",
    "### ğŸ› ï¸ Advanced CSS Selectors\n",
    "\n",
    "**More powerful selection techniques:**\n",
    "```css\n",
    "/* Attribute selectors */\n",
    "input[type=\"email\"]           /* Input with type email */\n",
    "a[href^=\"https\"]             /* Links starting with https */\n",
    "div[class*=\"product\"]        /* Divs with \"product\" in class */\n",
    "\n",
    "/* Pseudo-selectors */\n",
    "li:first-child              /* First list item */\n",
    "tr:nth-child(odd)           /* Odd table rows */\n",
    "p:contains(\"price\")         /* Paragraphs containing \"price\" */\n",
    "\n",
    "/* Combinators */\n",
    "div > p                     /* Direct child paragraphs */\n",
    "h2 + p                      /* Paragraph immediately after h2 */\n",
    "h2 ~ p                      /* All paragraphs after h2 */\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16dc2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ADVANCED SCRAPING: PAGINATION & SESSIONS\n",
      "=============================================\n",
      "\n",
      "ğŸ“„ Scraping page 1...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/1/...\n",
      "  âœ… Success! Status: 200\n",
      "  âœ… Extracted 10 quotes from page 1\n",
      "  â³ Waiting 1 seconds before next request...\n",
      "\n",
      "ğŸ“„ Scraping page 2...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/2/...\n",
      "  âœ… Success! Status: 200\n",
      "  âœ… Extracted 10 quotes from page 2\n",
      "  â³ Waiting 1 seconds before next request...\n",
      "\n",
      "ğŸ“„ Scraping page 3...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/3/...\n",
      "  âœ… Success! Status: 200\n",
      "  âœ… Extracted 10 quotes from page 3\n",
      "\n",
      "ğŸ“Š COMPREHENSIVE SCRAPING RESULTS:\n",
      "========================================\n",
      "Total quotes scraped: 30\n",
      "Pages scraped: 3\n",
      "Unique authors: 20\n",
      "\n",
      "Top 5 authors by quote count:\n",
      "  â€¢ Albert Einstein: 6 quotes\n",
      "  â€¢ J.K. Rowling: 3 quotes\n",
      "  â€¢ Marilyn Monroe: 2 quotes\n",
      "  â€¢ Dr. Seuss: 2 quotes\n",
      "  â€¢ Bob Marley: 2 quotes\n",
      "\n",
      "Top 5 most common tags:\n",
      "  â€¢ life: 7 occurrences\n",
      "  â€¢ love: 6 occurrences\n",
      "  â€¢ inspirational: 5 occurrences\n",
      "  â€¢ humor: 4 occurrences\n",
      "  â€¢ friends: 3 occurrences\n",
      "\n",
      "Quotes per page:\n",
      "  â€¢ Page 1: 10 quotes\n",
      "  â€¢ Page 2: 10 quotes\n",
      "  â€¢ Page 3: 10 quotes\n",
      "\n",
      "ğŸ¯ ADVANCED CSS SELECTORS DEMO\n",
      "===================================\n",
      "Advanced selector examples:\n",
      "Electronics products: 1\n",
      "Featured products: 1\n",
      "Product titles: ['Laptop', 'Python Programming']\n",
      "First product title: Laptop\n",
      "USD prices: ['$999', '$39']\n",
      "\n",
      "ğŸ’¡ Key Takeaways:\n",
      "  â€¢ Always add delays between requests\n",
      "  â€¢ Handle pagination systematically\n",
      "  â€¢ Use advanced selectors for precise targeting\n",
      "  â€¢ Monitor for anti-bot measures\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Advanced techniques - Pagination and session handling\n",
    "print(\"ğŸ”„ ADVANCED SCRAPING: PAGINATION & SESSIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def scrape_multiple_pages(base_url, max_pages=3, delay=1):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages with proper pagination handling\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "    all_quotes = []\n",
    "    \n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}/page/{page_num}/\"\n",
    "        print(f\"\\nğŸ“„ Scraping page {page_num}...\")\n",
    "        \n",
    "        response = safe_request(url, session)\n",
    "        if not response:\n",
    "            print(f\"  âŒ Failed to load page {page_num}\")\n",
    "            break\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Check if this page has quotes\n",
    "        quote_elements = soup.find_all('div', class_='quote')\n",
    "        if not quote_elements:\n",
    "            print(f\"  ğŸ“­ No quotes found on page {page_num}, stopping...\")\n",
    "            break\n",
    "            \n",
    "        # Extract quotes from this page\n",
    "        page_quotes = []\n",
    "        for quote_elem in quote_elements:\n",
    "            quote_data = {\n",
    "                'quote': quote_elem.find('span', class_='text').text,\n",
    "                'author': quote_elem.find('small', class_='author').text,\n",
    "                'tags': [tag.text for tag in quote_elem.find_all('a', class_='tag')],\n",
    "                'page': page_num\n",
    "            }\n",
    "            page_quotes.append(quote_data)\n",
    "            all_quotes.append(quote_data)\n",
    "        \n",
    "        print(f\"  âœ… Extracted {len(page_quotes)} quotes from page {page_num}\")\n",
    "        \n",
    "        # Check for \"Next\" button\n",
    "        next_btn = soup.find('li', class_='next')\n",
    "        if not next_btn:\n",
    "            print(f\"  ğŸ No 'Next' button found, reached end of quotes\")\n",
    "            break\n",
    "            \n",
    "        # Be respectful - add delay between requests\n",
    "        if page_num < max_pages:\n",
    "            print(f\"  â³ Waiting {delay} seconds before next request...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return all_quotes\n",
    "\n",
    "# Scrape multiple pages\n",
    "quotes_data = scrape_multiple_pages(\"http://quotes.toscrape.com\", max_pages=3)\n",
    "\n",
    "if quotes_data:\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(quotes_data)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š COMPREHENSIVE SCRAPING RESULTS:\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Total quotes scraped: {len(df)}\")\n",
    "    print(f\"Pages scraped: {df['page'].nunique()}\")\n",
    "    print(f\"Unique authors: {df['author'].nunique()}\")\n",
    "    \n",
    "    # Author statistics\n",
    "    author_counts = df['author'].value_counts()\n",
    "    print(f\"\\nTop 5 authors by quote count:\")\n",
    "    for author, count in author_counts.head().items():\n",
    "        print(f\"  â€¢ {author}: {count} quotes\")\n",
    "    \n",
    "    # Tag analysis\n",
    "    all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "    tag_counts = pd.Series(all_tags).value_counts()\n",
    "    print(f\"\\nTop 5 most common tags:\")\n",
    "    for tag, count in tag_counts.head().items():\n",
    "        print(f\"  â€¢ {tag}: {count} occurrences\")\n",
    "    \n",
    "    # Page distribution\n",
    "    page_counts = df['page'].value_counts().sort_index()\n",
    "    print(f\"\\nQuotes per page:\")\n",
    "    for page, count in page_counts.items():\n",
    "        print(f\"  â€¢ Page {page}: {count} quotes\")\n",
    "\n",
    "# Example: Advanced CSS selectors demonstration\n",
    "print(f\"\\nğŸ¯ ADVANCED CSS SELECTORS DEMO\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create sample HTML with complex structure\n",
    "complex_html = \"\"\"\n",
    "<div class=\"products\">\n",
    "    <div class=\"product\" data-category=\"electronics\">\n",
    "        <h3 class=\"title\">Laptop</h3>\n",
    "        <span class=\"price\" data-currency=\"USD\">$999</span>\n",
    "        <div class=\"rating\">\n",
    "            <span class=\"stars\">â˜…â˜…â˜…â˜…â˜†</span>\n",
    "            <span class=\"count\">(124 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div class=\"product featured\" data-category=\"books\">\n",
    "        <h3 class=\"title\">Python Programming</h3>\n",
    "        <span class=\"price\" data-currency=\"USD\">$39</span>\n",
    "        <div class=\"rating\">\n",
    "            <span class=\"stars\">â˜…â˜…â˜…â˜…â˜…</span>\n",
    "            <span class=\"count\">(89 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "\n",
    "print(\"Advanced selector examples:\")\n",
    "\n",
    "# Attribute selectors\n",
    "electronics = soup.select('div[data-category=\"electronics\"]')\n",
    "print(f\"Electronics products: {len(electronics)}\")\n",
    "\n",
    "# Class combinations\n",
    "featured_products = soup.select('.product.featured')\n",
    "print(f\"Featured products: {len(featured_products)}\")\n",
    "\n",
    "# Descendant selectors\n",
    "product_titles = soup.select('.product .title')\n",
    "print(f\"Product titles: {[title.text for title in product_titles]}\")\n",
    "\n",
    "# Pseudo-selectors (first, last, nth-child)\n",
    "first_product = soup.select('.product:first-child')\n",
    "print(f\"First product title: {first_product[0].find('h3').text if first_product else 'None'}\")\n",
    "\n",
    "# Advanced attribute selectors\n",
    "prices_usd = soup.select('span[data-currency=\"USD\"]')\n",
    "print(f\"USD prices: {[price.text for price in prices_usd]}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Takeaways:\")\n",
    "print(f\"  â€¢ Always add delays between requests\")\n",
    "print(f\"  â€¢ Handle pagination systematically\")\n",
    "print(f\"  â€¢ Use advanced selectors for precise targeting\")\n",
    "print(f\"  â€¢ Monitor for anti-bot measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576b15f",
   "metadata": {},
   "source": [
    "## Chapter 4: APIs vs Web Scraping ğŸ”„\n",
    "\n",
    "**Before scraping, always check if an API exists!** APIs are usually faster, more reliable, and more respectful.\n",
    "\n",
    "### ğŸ†š API vs Scraping Comparison\n",
    "\n",
    "| **Aspect** | **API** | **Web Scraping** |\n",
    "|------------|---------|------------------|\n",
    "| **Speed** | âš¡ Very fast | ğŸŒ Slower (HTML parsing) |\n",
    "| **Reliability** | ğŸ¯ Stable structure | ğŸ”„ Changes with website updates |\n",
    "| **Legal** | âœ… Usually permitted | âš–ï¸ Check terms of service |\n",
    "| **Rate Limits** | ğŸ“Š Clearly defined | ğŸš« Risk of being blocked |\n",
    "| **Data Format** | ğŸ“‹ Structured (JSON/XML) | ğŸ•¸ï¸ Unstructured (HTML) |\n",
    "| **Availability** | ğŸª Not always available | ğŸŒ Most websites accessible |\n",
    "\n",
    "### ğŸ” Finding APIs\n",
    "\n",
    "**How to discover if a site has an API:**\n",
    "1. **Check developer documentation** - Look for \"API\" or \"Developer\" pages\n",
    "2. **Inspect network traffic** - Use browser dev tools\n",
    "3. **Look for JSON responses** - Many sites use internal APIs\n",
    "4. **Search for \"[site name] API\"** - Google is your friend\n",
    "\n",
    "### ğŸ“Š Working with JSON APIs\n",
    "\n",
    "**JSON (JavaScript Object Notation) is the standard API format:**\n",
    "```python\n",
    "# Typical API response structure\n",
    "{\n",
    "    \"status\": \"success\",\n",
    "    \"data\": [\n",
    "        {\"id\": 1, \"name\": \"Product A\", \"price\": 29.99},\n",
    "        {\"id\": 2, \"name\": \"Product B\", \"price\": 39.99}\n",
    "    ],\n",
    "    \"pagination\": {\n",
    "        \"page\": 1,\n",
    "        \"total_pages\": 10,\n",
    "        \"per_page\": 20\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec4b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— WORKING WITH APIs AND JSON\n",
      "================================\n",
      "ğŸ“¡ EXAMPLE: JSONPLACEHOLDER API\n",
      "===================================\n",
      "âœ… Successfully fetched 5 posts\n",
      "\n",
      "Post data structure:\n",
      "   userId  id                                              title  \\\n",
      "0       1   1  sunt aut facere repellat provident occaecati e...   \n",
      "1       1   2                                       qui est esse   \n",
      "2       1   3  ea molestias quasi exercitationem repellat qui...   \n",
      "3       1   4                               eum et est occaecati   \n",
      "4       1   5                                 nesciunt quas odio   \n",
      "\n",
      "                                                body  \n",
      "0  quia et suscipit\\nsuscipit recusandae consequu...  \n",
      "1  est rerum tempore vitae\\nsequi sint nihil repr...  \n",
      "2  et iusto sed quo iure\\nvoluptatem occaecati om...  \n",
      "3  ullam et saepe reiciendis voluptatem adipisci\\...  \n",
      "4  repudiandae veniam quaerat sunt sed\\nalias aut...  \n",
      "\n",
      "ğŸ“Š MERGED DATA (Posts + User Info):\n",
      "                                               title           name  \\\n",
      "0  sunt aut facere repellat provident occaecati e...  Leanne Graham   \n",
      "1                                       qui est esse  Leanne Graham   \n",
      "2  ea molestias quasi exercitationem repellat qui...  Leanne Graham   \n",
      "3                               eum et est occaecati  Leanne Graham   \n",
      "4                                 nesciunt quas odio  Leanne Graham   \n",
      "\n",
      "               email  \n",
      "0  Sincere@april.biz  \n",
      "1  Sincere@april.biz  \n",
      "2  Sincere@april.biz  \n",
      "3  Sincere@april.biz  \n",
      "4  Sincere@april.biz  \n",
      "\n",
      "ğŸ†š API VS SCRAPING COMPARISON DEMO\n",
      "========================================\n",
      "API approach:\n",
      "  âœ… Fetched 10 items in 0.808 seconds\n",
      "  ğŸ“Š Clean, structured JSON data\n",
      "  ğŸ¯ No HTML parsing required\n",
      "\n",
      "ğŸ“‹ DATA STRUCTURE COMPARISON:\n",
      "===================================\n",
      "API Response (JSON):\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n",
      "\n",
      "HTML data (requires parsing):\n",
      "\n",
      "<div class=\"post\">\n",
      "    <h2 class=\"title\">sunt aut facere repellat</h2>\n",
      "    <div class=\"body\">quia et suscipit...</div>\n",
      "    <span class=\"author\">Leanne Graham</span>\n",
      "    <span class=\"user-id\" data-id=\"1\">1</span>\n",
      "</div>\n",
      "\n",
      "\n",
      "ğŸ”§ JSON DATA MANIPULATION\n",
      "==============================\n",
      "Nested JSON structure:\n",
      "{\n",
      "  \"user\": {\n",
      "    \"id\": 1,\n",
      "    \"profile\": {\n",
      "      \"name\": \"John Doe\",\n",
      "      \"contact\": {\n",
      "        \"email\": \"john@example.com\",\n",
      "        \"phone\": \"123-456-7890\"\n",
      "      }\n",
      "    },\n",
      "    \"posts\": [\n",
      "      {\n",
      "        \"id\": 1,\n",
      "        \"title\": \"First Post\",\n",
      "        \"likes\": 15\n",
      "      },\n",
      "      {\n",
      "        \"id\": 2,\n",
      "        \"title\": \"Second Post\",\n",
      "        \"likes\": 23\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Extracted information:\n",
      "  Name: John Doe\n",
      "  Email: john@example.com\n",
      "  Total likes: 38\n",
      "\n",
      "Flattened DataFrame:\n",
      "   user_id user_name        user_email  post_id   post_title  post_likes\n",
      "0        1  John Doe  john@example.com        1   First Post          15\n",
      "1        1  John Doe  john@example.com        2  Second Post          23\n",
      "\n",
      "ğŸ’¡ API BEST PRACTICES:\n",
      "  â€¢ Always check API documentation first\n",
      "  â€¢ Respect rate limits and use API keys\n",
      "  â€¢ Handle pagination in API responses\n",
      "  â€¢ Cache API responses when appropriate\n",
      "  â€¢ Fall back to scraping only when APIs aren't available\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Working with APIs and JSON data\n",
    "print(\"ğŸ”— WORKING WITH APIs AND JSON\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "def fetch_api_data(url, params=None):\n",
    "    \"\"\"\n",
    "    Fetch data from a JSON API with proper error handling\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if response is JSON\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if 'application/json' in content_type:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"âš ï¸ Response is not JSON: {content_type}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ API request failed: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ Invalid JSON response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Using a public API (JSONPlaceholder - a fake API for testing)\n",
    "print(\"ğŸ“¡ EXAMPLE: JSONPLACEHOLDER API\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Fetch sample posts from JSONPlaceholder\n",
    "posts_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "posts_data = fetch_api_data(posts_url, params={'_limit': 5})\n",
    "\n",
    "if posts_data:\n",
    "    print(f\"âœ… Successfully fetched {len(posts_data)} posts\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    print(f\"\\nPost data structure:\")\n",
    "    print(posts_df.head())\n",
    "    \n",
    "    # Fetch users data to join with posts\n",
    "    users_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "    users_data = fetch_api_data(users_url)\n",
    "    \n",
    "    if users_data:\n",
    "        users_df = pd.DataFrame(users_data)\n",
    "        \n",
    "        # Join posts with user information\n",
    "        merged_df = posts_df.merge(users_df[['id', 'name', 'email']], \n",
    "                                  left_on='userId', right_on='id', \n",
    "                                  suffixes=('_post', '_user'))\n",
    "        \n",
    "        print(f\"\\nğŸ“Š MERGED DATA (Posts + User Info):\")\n",
    "        print(merged_df[['title', 'name', 'email']].head())\n",
    "\n",
    "# Example: Comparing API vs Scraping approaches\n",
    "print(f\"\\nğŸ†š API VS SCRAPING COMPARISON DEMO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate API data extraction\n",
    "api_start_time = time.time()\n",
    "api_data = fetch_api_data(\"https://jsonplaceholder.typicode.com/posts\", {'_limit': 10})\n",
    "api_end_time = time.time()\n",
    "\n",
    "if api_data:\n",
    "    api_processing_time = api_end_time - api_start_time\n",
    "    print(f\"API approach:\")\n",
    "    print(f\"  âœ… Fetched {len(api_data)} items in {api_processing_time:.3f} seconds\")\n",
    "    print(f\"  ğŸ“Š Clean, structured JSON data\")\n",
    "    print(f\"  ğŸ¯ No HTML parsing required\")\n",
    "\n",
    "# Show the difference in data structure\n",
    "print(f\"\\nğŸ“‹ DATA STRUCTURE COMPARISON:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# API data (clean JSON)\n",
    "if api_data:\n",
    "    sample_api_item = api_data[0]\n",
    "    print(\"API Response (JSON):\")\n",
    "    print(json.dumps(sample_api_item, indent=2))\n",
    "\n",
    "print(f\"\\nHTML data (requires parsing):\")\n",
    "sample_html_data = \"\"\"\n",
    "<div class=\"post\">\n",
    "    <h2 class=\"title\">sunt aut facere repellat</h2>\n",
    "    <div class=\"body\">quia et suscipit...</div>\n",
    "    <span class=\"author\">Leanne Graham</span>\n",
    "    <span class=\"user-id\" data-id=\"1\">1</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "print(sample_html_data)\n",
    "\n",
    "# Demonstrate JSON manipulation\n",
    "print(f\"\\nğŸ”§ JSON DATA MANIPULATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Working with nested JSON (common in APIs)\n",
    "nested_json = {\n",
    "    \"user\": {\n",
    "        \"id\": 1,\n",
    "        \"profile\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"contact\": {\n",
    "                \"email\": \"john@example.com\",\n",
    "                \"phone\": \"123-456-7890\"\n",
    "            }\n",
    "        },\n",
    "        \"posts\": [\n",
    "            {\"id\": 1, \"title\": \"First Post\", \"likes\": 15},\n",
    "            {\"id\": 2, \"title\": \"Second Post\", \"likes\": 23}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Nested JSON structure:\")\n",
    "print(json.dumps(nested_json, indent=2))\n",
    "\n",
    "# Extract data from nested JSON\n",
    "user_name = nested_json['user']['profile']['name']\n",
    "user_email = nested_json['user']['profile']['contact']['email']\n",
    "total_likes = sum(post['likes'] for post in nested_json['user']['posts'])\n",
    "\n",
    "print(f\"\\nExtracted information:\")\n",
    "print(f\"  Name: {user_name}\")\n",
    "print(f\"  Email: {user_email}\")\n",
    "print(f\"  Total likes: {total_likes}\")\n",
    "\n",
    "# Flatten nested JSON for DataFrame\n",
    "flattened_data = []\n",
    "for post in nested_json['user']['posts']:\n",
    "    flat_record = {\n",
    "        'user_id': nested_json['user']['id'],\n",
    "        'user_name': nested_json['user']['profile']['name'],\n",
    "        'user_email': nested_json['user']['profile']['contact']['email'],\n",
    "        'post_id': post['id'],\n",
    "        'post_title': post['title'],\n",
    "        'post_likes': post['likes']\n",
    "    }\n",
    "    flattened_data.append(flat_record)\n",
    "\n",
    "flat_df = pd.DataFrame(flattened_data)\n",
    "print(f\"\\nFlattened DataFrame:\")\n",
    "print(flat_df)\n",
    "\n",
    "print(f\"\\nğŸ’¡ API BEST PRACTICES:\")\n",
    "print(f\"  â€¢ Always check API documentation first\")\n",
    "print(f\"  â€¢ Respect rate limits and use API keys\")\n",
    "print(f\"  â€¢ Handle pagination in API responses\")\n",
    "print(f\"  â€¢ Cache API responses when appropriate\")\n",
    "print(f\"  â€¢ Fall back to scraping only when APIs aren't available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd1bd1",
   "metadata": {},
   "source": [
    "## Chapter 5: Summary & Best Practices ğŸ¯\n",
    "\n",
    "### ğŸ† Key Takeaways\n",
    "\n",
    "**You've learned the fundamentals of web scraping!** Here's what we covered:\n",
    "\n",
    "1. **ğŸ” HTML Structure** - Understanding the anatomy of web pages\n",
    "2. **ğŸ› ï¸ Basic Scraping** - Using requests and BeautifulSoup\n",
    "3. **ğŸš€ Advanced Techniques** - Pagination, sessions, and error handling\n",
    "4. **ğŸ“Š Data Processing** - Converting scraped data to DataFrames\n",
    "5. **ğŸ”Œ APIs vs Scraping** - Knowing when to use each approach\n",
    "\n",
    "### âš–ï¸ Legal and Ethical Guidelines\n",
    "\n",
    "**ğŸš¨ Always Remember:**\n",
    "- **Check robots.txt** - Respect website guidelines\n",
    "- **Read Terms of Service** - Understand legal restrictions\n",
    "- **Be respectful** - Don't overload servers with requests\n",
    "- **Add delays** - Space out your requests (1-2 seconds minimum)\n",
    "- **Use APIs when available** - They're designed for data access\n",
    "\n",
    "### ğŸ›¡ï¸ Best Practices Checklist\n",
    "\n",
    "**Before You Scrape:**\n",
    "- [ ] Check if an API exists first\n",
    "- [ ] Read the website's robots.txt\n",
    "- [ ] Review terms of service\n",
    "- [ ] Test with small samples first\n",
    "\n",
    "**During Scraping:**\n",
    "- [ ] Use proper headers (User-Agent, etc.)\n",
    "- [ ] Implement rate limiting and delays\n",
    "- [ ] Handle errors gracefully\n",
    "- [ ] Monitor for blocking/captchas\n",
    "- [ ] Cache responses when appropriate\n",
    "\n",
    "**After Scraping:**\n",
    "- [ ] Clean and validate your data\n",
    "- [ ] Store data efficiently\n",
    "- [ ] Document your scraping process\n",
    "- [ ] Schedule updates responsibly\n",
    "\n",
    "### ğŸ”® Next Steps\n",
    "\n",
    "**To become a scraping expert, explore:**\n",
    "\n",
    "1. **Advanced Tools:**\n",
    "   - **Selenium/Playwright** - For JavaScript-heavy sites\n",
    "   - **Scrapy** - Professional scraping framework\n",
    "   - **Beautiful Soup alternatives** - lxml, html.parser\n",
    "\n",
    "2. **Handling Challenges:**\n",
    "   - **CAPTCHAs** - Detection and solving\n",
    "   - **IP blocking** - Proxy rotation\n",
    "   - **Dynamic content** - Browser automation\n",
    "   - **Anti-bot measures** - Stealth techniques\n",
    "\n",
    "3. **Data Pipeline:**\n",
    "   - **Storage** - Databases, files, cloud storage\n",
    "   - **Processing** - Data cleaning and validation\n",
    "   - **Monitoring** - Automated scraping jobs\n",
    "   - **Visualization** - Dashboards and reports\n",
    "\n",
    "### ğŸ“š Recommended Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Documentation](https://docs.python-requests.org/)\n",
    "- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "\n",
    "**Practice Sites:**\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/) - Beginner friendly\n",
    "- [Books to Scrape](http://books.toscrape.com/) - More complex structure\n",
    "- [Scrape This Site](https://scrapethissite.com/) - Various challenges\n",
    "\n",
    "**Remember:** Web scraping is a powerful tool for data collection, but with great power comes great responsibility! Always scrape ethically and respectfully. ğŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb16a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ PRACTICAL EXERCISE\n",
      "=========================\n",
      "ğŸ“š BOOKS SCRAPING CHALLENGE\n",
      "Your mission: Scrape book data from http://books.toscrape.com/\n",
      "\n",
      "What to extract:\n",
      "  â€¢ Book titles\n",
      "  â€¢ Prices\n",
      "  â€¢ Star ratings\n",
      "  â€¢ Availability status\n",
      "\n",
      "ğŸ’¡ HINTS:\n",
      "  â€¢ Use inspector to examine the HTML structure\n",
      "  â€¢ Look for class names like 'product_pod'\n",
      "  â€¢ Ratings are in class names like 'star-rating Three'\n",
      "  â€¢ Prices are in <p class='price_color'>\n",
      "\n",
      "ğŸš€ BONUS CHALLENGES:\n",
      "  â€¢ Scrape multiple pages\n",
      "  â€¢ Calculate average price by rating\n",
      "  â€¢ Find the most expensive book\n",
      "  â€¢ Create a visualization of price distribution\n",
      "\n",
      "ğŸ“ CODE TEMPLATE:\n",
      "\n",
      "# Your solution here:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "# 1. Make request to books.toscrape.com\n",
      "# 2. Parse HTML with BeautifulSoup  \n",
      "# 3. Find book containers\n",
      "# 4. Extract title, price, rating, availability\n",
      "# 5. Convert to DataFrame\n",
      "# 6. Analyze the data\n",
      "\n",
      "# url = \"http://books.toscrape.com/\"\n",
      "# response = requests.get(url)\n",
      "# soup = BeautifulSoup(response.content, 'html.parser')\n",
      "# ... your code here ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ’¼ REAL-WORLD SCRAPING WORKFLOW\n",
      "==================================================\n",
      "  1. ğŸ¯ Define your objective - What data do you need?\n",
      "  2. ğŸ” Explore the website - Understand the structure\n",
      "  3. ğŸ“‹ Check legal requirements - robots.txt, ToS\n",
      "  4. ğŸ› ï¸ Choose your tools - requests + BeautifulSoup or alternatives\n",
      "  5. ğŸ§ª Start small - Test with single pages first\n",
      "  6. ğŸ”„ Scale up - Handle pagination and multiple pages\n",
      "  7. ğŸ§¹ Clean data - Remove duplicates, handle missing values\n",
      "  8. ğŸ’¾ Store results - Database, CSV, or other formats\n",
      "  9. ğŸ”„ Automate - Schedule regular scraping if needed\n",
      "  10. ğŸ“Š Analyze - Turn data into insights!\n",
      "\n",
      "ğŸ‰ CONGRATULATIONS!\n",
      "You now have the foundation to scrape data from the web!\n",
      "Remember: Practice makes perfect, and always scrape responsibly! ğŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ PRACTICAL EXERCISE: Your Turn to Scrape!\n",
    "print(\"ğŸ¯ PRACTICAL EXERCISE\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Exercise: Scrape book information from books.toscrape.com\n",
    "# This is a more complex scraping challenge!\n",
    "\n",
    "def scrape_books_challenge():\n",
    "    \"\"\"\n",
    "    Challenge: Scrape book data from books.toscrape.com\n",
    "    Try to extract: title, price, rating, availability\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ“š BOOKS SCRAPING CHALLENGE\")\n",
    "    print(\"Your mission: Scrape book data from http://books.toscrape.com/\")\n",
    "    print(\"\\nWhat to extract:\")\n",
    "    print(\"  â€¢ Book titles\")\n",
    "    print(\"  â€¢ Prices\") \n",
    "    print(\"  â€¢ Star ratings\")\n",
    "    print(\"  â€¢ Availability status\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ HINTS:\")\n",
    "    print(\"  â€¢ Use inspector to examine the HTML structure\")\n",
    "    print(\"  â€¢ Look for class names like 'product_pod'\")\n",
    "    print(\"  â€¢ Ratings are in class names like 'star-rating Three'\")\n",
    "    print(\"  â€¢ Prices are in <p class='price_color'>\")\n",
    "    \n",
    "    print(\"\\nğŸš€ BONUS CHALLENGES:\")\n",
    "    print(\"  â€¢ Scrape multiple pages\")\n",
    "    print(\"  â€¢ Calculate average price by rating\")\n",
    "    print(\"  â€¢ Find the most expensive book\")\n",
    "    print(\"  â€¢ Create a visualization of price distribution\")\n",
    "    \n",
    "    print(\"\\nğŸ“ CODE TEMPLATE:\")\n",
    "    template_code = '''\n",
    "# Your solution here:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Make request to books.toscrape.com\n",
    "# 2. Parse HTML with BeautifulSoup  \n",
    "# 3. Find book containers\n",
    "# 4. Extract title, price, rating, availability\n",
    "# 5. Convert to DataFrame\n",
    "# 6. Analyze the data\n",
    "\n",
    "# url = \"http://books.toscrape.com/\"\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# ... your code here ...\n",
    "'''\n",
    "    print(template_code)\n",
    "\n",
    "# Run the challenge setup\n",
    "scrape_books_challenge()\n",
    "\n",
    "# Example solution (commented out - let students try first!)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ’¼ REAL-WORLD SCRAPING WORKFLOW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "workflow_steps = [\n",
    "    \"1. ğŸ¯ Define your objective - What data do you need?\",\n",
    "    \"2. ğŸ” Explore the website - Understand the structure\", \n",
    "    \"3. ğŸ“‹ Check legal requirements - robots.txt, ToS\",\n",
    "    \"4. ğŸ› ï¸ Choose your tools - requests + BeautifulSoup or alternatives\",\n",
    "    \"5. ğŸ§ª Start small - Test with single pages first\",\n",
    "    \"6. ğŸ”„ Scale up - Handle pagination and multiple pages\", \n",
    "    \"7. ğŸ§¹ Clean data - Remove duplicates, handle missing values\",\n",
    "    \"8. ğŸ’¾ Store results - Database, CSV, or other formats\",\n",
    "    \"9. ğŸ”„ Automate - Schedule regular scraping if needed\",\n",
    "    \"10. ğŸ“Š Analyze - Turn data into insights!\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ CONGRATULATIONS!\")\n",
    "print(\"You now have the foundation to scrape data from the web!\")\n",
    "print(\"Remember: Practice makes perfect, and always scrape responsibly! ğŸŒŸ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
