{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e28767",
   "metadata": {},
   "source": [
    "# Web Scraping Fundamentals: From HTML to Data üåê\n",
    "\n",
    "## What is Web Scraping? ü§î\n",
    "\n",
    "**Web scraping** is the process of automatically extracting data from websites. Think of it as teaching your computer to \"read\" web pages like a human would, but much faster and more systematically.\n",
    "\n",
    "**Imagine this scenario:** You want to collect product prices from an e-commerce site, track news headlines, or gather research data from multiple websites. Instead of manually copying and pasting for hours, web scraping automates this process!\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **üîç The anatomy of web pages** - HTML structure and how data is organized\n",
    "2. **üõ†Ô∏è Essential scraping tools** - requests, BeautifulSoup, and modern alternatives\n",
    "3. **üìä Data extraction techniques** - Finding and extracting specific information\n",
    "4. **‚öñÔ∏è Legal and ethical considerations** - Scraping responsibly and respectfully\n",
    "5. **üöÄ Advanced concepts** - Handling dynamic content, APIs, and challenges\n",
    "6. **üíº Real-world applications** - Practical examples and use cases\n",
    "\n",
    "## üåü Why Web Scraping Matters\n",
    "\n",
    "**In Data Science:**\n",
    "- **Data Collection:** Gather datasets for analysis and machine learning\n",
    "- **Market Research:** Monitor competitors, prices, and trends\n",
    "- **Automation:** Replace manual data entry with automated processes\n",
    "\n",
    "**In Business:**\n",
    "- **Lead Generation:** Extract contact information and business data\n",
    "- **Content Aggregation:** Collect news, reviews, and social media posts\n",
    "- **Monitoring:** Track mentions, reviews, and brand sentiment\n",
    "\n",
    "**The Big Picture:** Web scraping democratizes data access, turning the entire web into your database! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde20a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Web Scraping Toolkit Ready!\n",
      "üìö Libraries loaded:\n",
      "  ‚Ä¢ requests: HTTP requests and web communication\n",
      "  ‚Ä¢ BeautifulSoup: HTML parsing and navigation\n",
      "  ‚Ä¢ pandas: Data manipulation and analysis\n",
      "  ‚Ä¢ json: Handle JSON data from APIs\n",
      "  ‚Ä¢ urllib: URL handling and robots.txt checking\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization and data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"üîß Web Scraping Toolkit Ready!\")\n",
    "print(\"üìö Libraries loaded:\")\n",
    "print(\"  ‚Ä¢ requests: HTTP requests and web communication\")\n",
    "print(\"  ‚Ä¢ BeautifulSoup: HTML parsing and navigation\")\n",
    "print(\"  ‚Ä¢ pandas: Data manipulation and analysis\")\n",
    "print(\"  ‚Ä¢ json: Handle JSON data from APIs\")\n",
    "print(\"  ‚Ä¢ urllib: URL handling and robots.txt checking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912eb68",
   "metadata": {},
   "source": [
    "## Chapter 1: Understanding HTML Structure üìù\n",
    "\n",
    "**Before we scrape, we need to understand what we're scraping!** HTML (HyperText Markup Language) is the backbone of web pages.\n",
    "\n",
    "### üèóÔ∏è HTML Anatomy\n",
    "\n",
    "**Think of HTML like a document outline:**\n",
    "- **Tags:** Define structure (headings, paragraphs, lists)\n",
    "- **Attributes:** Provide additional information (id, class, href)\n",
    "- **Content:** The actual text and data we want to extract\n",
    "\n",
    "**Common HTML Elements:**\n",
    "```html\n",
    "<html>                    <!-- Root element -->\n",
    "  <head>                  <!-- Metadata -->\n",
    "    <title>Page Title</title>\n",
    "  </head>\n",
    "  <body>                  <!-- Visible content -->\n",
    "    <h1 id=\"main-title\">Heading</h1>\n",
    "    <p class=\"description\">Paragraph text</p>\n",
    "    <div class=\"container\">\n",
    "      <ul>                <!-- Unordered list -->\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "    <a href=\"https://example.com\">Link</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### üéØ CSS Selectors: Your Navigation Tool\n",
    "\n",
    "**CSS selectors help us find specific elements:**\n",
    "- **Element:** `p` (all paragraphs)\n",
    "- **Class:** `.description` (elements with class=\"description\")\n",
    "- **ID:** `#main-title` (element with id=\"main-title\")\n",
    "- **Descendant:** `div p` (paragraphs inside divs)\n",
    "- **Attribute:** `a[href]` (links with href attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6631f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê MAKING HTTP REQUESTS\n",
      "==============================\n",
      "üìÑ HTML PARSING DEMONSTRATION\n",
      "===================================\n",
      "Product Title: Amazing Laptop\n",
      "Price: $999.99\n",
      "Description: High-performance laptop with 16GB RAM\n",
      "Features: ['16GB RAM', '512GB SSD', 'Intel i7 Processor']\n",
      "\n",
      "Reviews:\n",
      "  1. 5 stars - Excellent laptop!\n",
      "  2. 4 stars - Great value for money.\n",
      "\n",
      "üéØ Key BeautifulSoup Methods:\n",
      "  ‚Ä¢ find() - Gets the first matching element\n",
      "  ‚Ä¢ find_all() - Gets all matching elements\n",
      "  ‚Ä¢ .text - Extracts text content\n",
      "  ‚Ä¢ .get() - Gets attribute values\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Making your first HTTP request\n",
    "print(\"üåê MAKING HTTP REQUESTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Example HTML content (simulating a simple webpage)\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Product Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 id=\"product-title\">Amazing Laptop</h1>\n",
    "        <p class=\"price\">$999.99</p>\n",
    "        <p class=\"description\">High-performance laptop with 16GB RAM</p>\n",
    "        <ul class=\"features\">\n",
    "            <li>16GB RAM</li>\n",
    "            <li>512GB SSD</li>\n",
    "            <li>Intel i7 Processor</li>\n",
    "        </ul>\n",
    "        <div class=\"reviews\">\n",
    "            <div class=\"review\">\n",
    "                <span class=\"rating\">5 stars</span>\n",
    "                <p class=\"comment\">Excellent laptop!</p>\n",
    "            </div>\n",
    "            <div class=\"review\">\n",
    "                <span class=\"rating\">4 stars</span>\n",
    "                <p class=\"comment\">Great value for money.</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"üìÑ HTML PARSING DEMONSTRATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Extract different types of information\n",
    "product_title = soup.find('h1', id='product-title').text\n",
    "price = soup.find('p', class_='price').text\n",
    "description = soup.find('p', class_='description').text\n",
    "\n",
    "print(f\"Product Title: {product_title}\")\n",
    "print(f\"Price: {price}\")\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "# Extract multiple elements (features list)\n",
    "features = soup.find_all('li')\n",
    "feature_list = [feature.text for feature in features]\n",
    "print(f\"Features: {feature_list}\")\n",
    "\n",
    "# Extract reviews\n",
    "reviews = soup.find_all('div', class_='review')\n",
    "review_data = []\n",
    "for review in reviews:\n",
    "    rating = review.find('span', class_='rating').text\n",
    "    comment = review.find('p', class_='comment').text\n",
    "    review_data.append({'rating': rating, 'comment': comment})\n",
    "\n",
    "print(f\"\\nReviews:\")\n",
    "for i, review in enumerate(review_data, 1):\n",
    "    print(f\"  {i}. {review['rating']} - {review['comment']}\")\n",
    "\n",
    "print(f\"\\nüéØ Key BeautifulSoup Methods:\")\n",
    "print(f\"  ‚Ä¢ find() - Gets the first matching element\")\n",
    "print(f\"  ‚Ä¢ find_all() - Gets all matching elements\")\n",
    "print(f\"  ‚Ä¢ .text - Extracts text content\")\n",
    "print(f\"  ‚Ä¢ .get() - Gets attribute values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715eefa",
   "metadata": {},
   "source": [
    "## Chapter 2: Real-World Web Scraping üåç\n",
    "\n",
    "**Now let's scrape actual websites!** We'll start with a simple, scraper-friendly site and demonstrate best practices.\n",
    "\n",
    "### üõ°Ô∏è Best Practices for Responsible Scraping\n",
    "\n",
    "**Before scraping any website:**\n",
    "1. **Check robots.txt** - See what's allowed: `website.com/robots.txt`\n",
    "2. **Use proper headers** - Identify yourself as a legitimate user\n",
    "3. **Respect rate limits** - Don't overwhelm servers with requests\n",
    "4. **Handle errors gracefully** - Networks fail, pages change\n",
    "5. **Consider APIs first** - Many sites offer data through APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd60d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß PROFESSIONAL WEB SCRAPING SETUP\n",
      "========================================\n",
      "\n",
      "üìö EXAMPLE: SCRAPING QUOTES FROM QUOTES.TOSCRAPE.COM\n",
      "=======================================================\n",
      "ü§ñ Checking robots.txt...\n",
      "  Can fetch http://quotes.toscrape.com/page/1/: True\n",
      "  Attempting request to: http://quotes.toscrape.com/page/1/...\n",
      "  ‚úÖ Success! Status: 200\n",
      "\n",
      "üìñ Found 10 quotes:\n",
      "----------------------------------------\n",
      "1. \"‚ÄúThe world as we have created it is a process of our thinking. It cannot be changed without changing...\" - Albert Einstein\n",
      "   Tags: change, deep-thoughts, thinking, world\n",
      "\n",
      "2. \"‚ÄúIt is our choices, Harry, that show what we truly are, far more than our abilities.‚Äù...\" - J.K. Rowling\n",
      "   Tags: abilities, choices\n",
      "\n",
      "3. \"‚ÄúThere are only two ways to live your life. One is as though nothing is a miracle. The other is as t...\" - Albert Einstein\n",
      "   Tags: inspirational, life, live, miracle, miracles\n",
      "\n",
      "üìä SCRAPED DATA SUMMARY:\n",
      "  ‚Ä¢ Total quotes: 10\n",
      "  ‚Ä¢ Unique authors: 8\n",
      "  ‚Ä¢ Most common tags: {'inspirational': 3, 'life': 2, 'humor': 2}\n",
      "\n",
      "üìã Sample DataFrame:\n",
      "                                               quote           author  \\\n",
      "0  ‚ÄúThe world as we have created it is a process ...  Albert Einstein   \n",
      "1  ‚ÄúIt is our choices, Harry, that show what we t...     J.K. Rowling   \n",
      "2  ‚ÄúThere are only two ways to live your life. On...  Albert Einstein   \n",
      "3  ‚ÄúThe person, be it gentleman or lady, who has ...      Jane Austen   \n",
      "4  ‚ÄúImperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
      "\n",
      "                                       tags_str  \n",
      "0        change, deep-thoughts, thinking, world  \n",
      "1                            abilities, choices  \n",
      "2  inspirational, life, live, miracle, miracles  \n",
      "3              aliteracy, books, classic, humor  \n",
      "4                    be-yourself, inspirational  \n"
     ]
    }
   ],
   "source": [
    "# Example 2: Professional web scraping with proper headers and error handling\n",
    "print(\"üîß PROFESSIONAL WEB SCRAPING SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a properly configured requests session\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Set headers to appear like a real browser\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    })\n",
    "    \n",
    "    return session\n",
    "\n",
    "def safe_request(url, session, timeout=10, retries=3):\n",
    "    \"\"\"Make a safe HTTP request with retries and error handling\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"  Attempting request to: {url[:50]}...\")\n",
    "            response = session.get(url, timeout=timeout)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            print(f\"  ‚úÖ Success! Status: {response.status_code}\")\n",
    "            return response\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  ‚è∞ Timeout on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  üîå Connection error on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"  üö´ HTTP error: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Unexpected error: {e}\")\n",
    "        \n",
    "        if attempt < retries - 1:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"  ‚è≥ Waiting {wait_time} seconds before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"  ‚ùå Failed to fetch {url} after {retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def check_robots_txt(base_url):\n",
    "    \"\"\"Check robots.txt for scraping permissions\"\"\"\n",
    "    try:\n",
    "        robots_url = urllib.parse.urljoin(base_url, '/robots.txt')\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except:\n",
    "        print(f\"  ‚ö†Ô∏è Could not fetch robots.txt for {base_url}\")\n",
    "        return None\n",
    "\n",
    "# Example: Scraping quotes from a test website\n",
    "print(\"\\nüìö EXAMPLE: SCRAPING QUOTES FROM QUOTES.TOSCRAPE.COM\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# This is a website specifically designed for scraping practice\n",
    "base_url = \"http://quotes.toscrape.com\"\n",
    "url = f\"{base_url}/page/1/\"\n",
    "\n",
    "# Check robots.txt\n",
    "print(\"ü§ñ Checking robots.txt...\")\n",
    "robots = check_robots_txt(base_url)\n",
    "if robots:\n",
    "    user_agent = '*'\n",
    "    can_fetch = robots.can_fetch(user_agent, url)\n",
    "    print(f\"  Can fetch {url}: {can_fetch}\")\n",
    "\n",
    "# Create session and make request\n",
    "session = create_session()\n",
    "response = safe_request(url, session)\n",
    "\n",
    "if response:\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract quotes data\n",
    "    quotes_data = []\n",
    "    quote_elements = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    print(f\"\\nüìñ Found {len(quote_elements)} quotes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, quote_elem in enumerate(quote_elements, 1):\n",
    "        # Extract quote text\n",
    "        quote_text = quote_elem.find('span', class_='text').text\n",
    "        \n",
    "        # Extract author\n",
    "        author = quote_elem.find('small', class_='author').text\n",
    "        \n",
    "        # Extract tags\n",
    "        tag_elements = quote_elem.find_all('a', class_='tag')\n",
    "        tags = [tag.text for tag in tag_elements]\n",
    "        \n",
    "        quote_data = {\n",
    "            'quote': quote_text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        }\n",
    "        quotes_data.append(quote_data)\n",
    "        \n",
    "        # Display first few quotes\n",
    "        if i <= 3:\n",
    "            print(f\"{i}. \\\"{quote_text[:100]}...\\\" - {author}\")\n",
    "            print(f\"   Tags: {', '.join(tags)}\")\n",
    "            print()\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    quotes_df = pd.DataFrame(quotes_data)\n",
    "    print(f\"üìä SCRAPED DATA SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Total quotes: {len(quotes_df)}\")\n",
    "    print(f\"  ‚Ä¢ Unique authors: {quotes_df['author'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Most common tags: {pd.Series([tag for tags in quotes_df['tags'] for tag in tags]).value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(f\"\\nüìã Sample DataFrame:\")\n",
    "    quotes_df['tags_str'] = quotes_df['tags'].apply(lambda x: ', '.join(x))\n",
    "    display_df = quotes_df[['quote', 'author', 'tags_str']].copy()\n",
    "    display_df['quote'] = display_df['quote'].str[:50] + '...'\n",
    "    print(display_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Could not fetch the webpage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2425b5",
   "metadata": {},
   "source": [
    "## Chapter 3: Advanced Scraping Techniques üöÄ\n",
    "\n",
    "**Beyond basic scraping:** Real websites often require more sophisticated approaches.\n",
    "\n",
    "### üîÑ Handling Dynamic Content\n",
    "\n",
    "**Many modern websites load content with JavaScript.** For these sites, you might need:\n",
    "\n",
    "1. **Selenium WebDriver** - Controls a real browser\n",
    "2. **Playwright** - Modern browser automation\n",
    "3. **requests-html** - JavaScript support for requests\n",
    "\n",
    "### üìã Forms and Sessions\n",
    "\n",
    "**Some data requires:**\n",
    "- **Login authentication** - Maintaining sessions\n",
    "- **Form submissions** - POST requests with CSRF tokens\n",
    "- **Pagination** - Following \"next page\" links\n",
    "- **AJAX requests** - API calls that load data dynamically\n",
    "\n",
    "### üõ†Ô∏è Advanced CSS Selectors\n",
    "\n",
    "**More powerful selection techniques:**\n",
    "```css\n",
    "/* Attribute selectors */\n",
    "input[type=\"email\"]           /* Input with type email */\n",
    "a[href^=\"https\"]             /* Links starting with https */\n",
    "div[class*=\"product\"]        /* Divs with \"product\" in class */\n",
    "\n",
    "/* Pseudo-selectors */\n",
    "li:first-child              /* First list item */\n",
    "tr:nth-child(odd)           /* Odd table rows */\n",
    "p:contains(\"price\")         /* Paragraphs containing \"price\" */\n",
    "\n",
    "/* Combinators */\n",
    "div > p                     /* Direct child paragraphs */\n",
    "h2 + p                      /* Paragraph immediately after h2 */\n",
    "h2 ~ p                      /* All paragraphs after h2 */\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16dc2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ADVANCED SCRAPING: PAGINATION & SESSIONS\n",
      "=============================================\n",
      "\n",
      "üìÑ Scraping page 1...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/1/...\n",
      "  ‚úÖ Success! Status: 200\n",
      "  ‚úÖ Extracted 10 quotes from page 1\n",
      "  ‚è≥ Waiting 1 seconds before next request...\n",
      "\n",
      "üìÑ Scraping page 2...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/2/...\n",
      "  ‚úÖ Success! Status: 200\n",
      "  ‚úÖ Extracted 10 quotes from page 2\n",
      "  ‚è≥ Waiting 1 seconds before next request...\n",
      "\n",
      "üìÑ Scraping page 3...\n",
      "  Attempting request to: http://quotes.toscrape.com/page/3/...\n",
      "  ‚úÖ Success! Status: 200\n",
      "  ‚úÖ Extracted 10 quotes from page 3\n",
      "\n",
      "üìä COMPREHENSIVE SCRAPING RESULTS:\n",
      "========================================\n",
      "Total quotes scraped: 30\n",
      "Pages scraped: 3\n",
      "Unique authors: 20\n",
      "\n",
      "Top 5 authors by quote count:\n",
      "  ‚Ä¢ Albert Einstein: 6 quotes\n",
      "  ‚Ä¢ J.K. Rowling: 3 quotes\n",
      "  ‚Ä¢ Marilyn Monroe: 2 quotes\n",
      "  ‚Ä¢ Dr. Seuss: 2 quotes\n",
      "  ‚Ä¢ Bob Marley: 2 quotes\n",
      "\n",
      "Top 5 most common tags:\n",
      "  ‚Ä¢ life: 7 occurrences\n",
      "  ‚Ä¢ love: 6 occurrences\n",
      "  ‚Ä¢ inspirational: 5 occurrences\n",
      "  ‚Ä¢ humor: 4 occurrences\n",
      "  ‚Ä¢ friends: 3 occurrences\n",
      "\n",
      "Quotes per page:\n",
      "  ‚Ä¢ Page 1: 10 quotes\n",
      "  ‚Ä¢ Page 2: 10 quotes\n",
      "  ‚Ä¢ Page 3: 10 quotes\n",
      "\n",
      "üéØ ADVANCED CSS SELECTORS DEMO\n",
      "===================================\n",
      "Advanced selector examples:\n",
      "Electronics products: 1\n",
      "Featured products: 1\n",
      "Product titles: ['Laptop', 'Python Programming']\n",
      "First product title: Laptop\n",
      "USD prices: ['$999', '$39']\n",
      "\n",
      "üí° Key Takeaways:\n",
      "  ‚Ä¢ Always add delays between requests\n",
      "  ‚Ä¢ Handle pagination systematically\n",
      "  ‚Ä¢ Use advanced selectors for precise targeting\n",
      "  ‚Ä¢ Monitor for anti-bot measures\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Advanced techniques - Pagination and session handling\n",
    "print(\"üîÑ ADVANCED SCRAPING: PAGINATION & SESSIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def scrape_multiple_pages(base_url, max_pages=3, delay=1):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages with proper pagination handling\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "    all_quotes = []\n",
    "    \n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}/page/{page_num}/\"\n",
    "        print(f\"\\nüìÑ Scraping page {page_num}...\")\n",
    "        \n",
    "        response = safe_request(url, session)\n",
    "        if not response:\n",
    "            print(f\"  ‚ùå Failed to load page {page_num}\")\n",
    "            break\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Check if this page has quotes\n",
    "        quote_elements = soup.find_all('div', class_='quote')\n",
    "        if not quote_elements:\n",
    "            print(f\"  üì≠ No quotes found on page {page_num}, stopping...\")\n",
    "            break\n",
    "            \n",
    "        # Extract quotes from this page\n",
    "        page_quotes = []\n",
    "        for quote_elem in quote_elements:\n",
    "            quote_data = {\n",
    "                'quote': quote_elem.find('span', class_='text').text,\n",
    "                'author': quote_elem.find('small', class_='author').text,\n",
    "                'tags': [tag.text for tag in quote_elem.find_all('a', class_='tag')],\n",
    "                'page': page_num\n",
    "            }\n",
    "            page_quotes.append(quote_data)\n",
    "            all_quotes.append(quote_data)\n",
    "        \n",
    "        print(f\"  ‚úÖ Extracted {len(page_quotes)} quotes from page {page_num}\")\n",
    "        \n",
    "        # Check for \"Next\" button\n",
    "        next_btn = soup.find('li', class_='next')\n",
    "        if not next_btn:\n",
    "            print(f\"  üèÅ No 'Next' button found, reached end of quotes\")\n",
    "            break\n",
    "            \n",
    "        # Be respectful - add delay between requests\n",
    "        if page_num < max_pages:\n",
    "            print(f\"  ‚è≥ Waiting {delay} seconds before next request...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return all_quotes\n",
    "\n",
    "# Scrape multiple pages\n",
    "quotes_data = scrape_multiple_pages(\"http://quotes.toscrape.com\", max_pages=3)\n",
    "\n",
    "if quotes_data:\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(quotes_data)\n",
    "    \n",
    "    print(f\"\\nüìä COMPREHENSIVE SCRAPING RESULTS:\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Total quotes scraped: {len(df)}\")\n",
    "    print(f\"Pages scraped: {df['page'].nunique()}\")\n",
    "    print(f\"Unique authors: {df['author'].nunique()}\")\n",
    "    \n",
    "    # Author statistics\n",
    "    author_counts = df['author'].value_counts()\n",
    "    print(f\"\\nTop 5 authors by quote count:\")\n",
    "    for author, count in author_counts.head().items():\n",
    "        print(f\"  ‚Ä¢ {author}: {count} quotes\")\n",
    "    \n",
    "    # Tag analysis\n",
    "    all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "    tag_counts = pd.Series(all_tags).value_counts()\n",
    "    print(f\"\\nTop 5 most common tags:\")\n",
    "    for tag, count in tag_counts.head().items():\n",
    "        print(f\"  ‚Ä¢ {tag}: {count} occurrences\")\n",
    "    \n",
    "    # Page distribution\n",
    "    page_counts = df['page'].value_counts().sort_index()\n",
    "    print(f\"\\nQuotes per page:\")\n",
    "    for page, count in page_counts.items():\n",
    "        print(f\"  ‚Ä¢ Page {page}: {count} quotes\")\n",
    "\n",
    "# Example: Advanced CSS selectors demonstration\n",
    "print(f\"\\nüéØ ADVANCED CSS SELECTORS DEMO\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create sample HTML with complex structure\n",
    "complex_html = \"\"\"\n",
    "<div class=\"products\">\n",
    "    <div class=\"product\" data-category=\"electronics\">\n",
    "        <h3 class=\"title\">Laptop</h3>\n",
    "        <span class=\"price\" data-currency=\"USD\">$999</span>\n",
    "        <div class=\"rating\">\n",
    "            <span class=\"stars\">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</span>\n",
    "            <span class=\"count\">(124 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div class=\"product featured\" data-category=\"books\">\n",
    "        <h3 class=\"title\">Python Programming</h3>\n",
    "        <span class=\"price\" data-currency=\"USD\">$39</span>\n",
    "        <div class=\"rating\">\n",
    "            <span class=\"stars\">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</span>\n",
    "            <span class=\"count\">(89 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "\n",
    "print(\"Advanced selector examples:\")\n",
    "\n",
    "# Attribute selectors\n",
    "electronics = soup.select('div[data-category=\"electronics\"]')\n",
    "print(f\"Electronics products: {len(electronics)}\")\n",
    "\n",
    "# Class combinations\n",
    "featured_products = soup.select('.product.featured')\n",
    "print(f\"Featured products: {len(featured_products)}\")\n",
    "\n",
    "# Descendant selectors\n",
    "product_titles = soup.select('.product .title')\n",
    "print(f\"Product titles: {[title.text for title in product_titles]}\")\n",
    "\n",
    "# Pseudo-selectors (first, last, nth-child)\n",
    "first_product = soup.select('.product:first-child')\n",
    "print(f\"First product title: {first_product[0].find('h3').text if first_product else 'None'}\")\n",
    "\n",
    "# Advanced attribute selectors\n",
    "prices_usd = soup.select('span[data-currency=\"USD\"]')\n",
    "print(f\"USD prices: {[price.text for price in prices_usd]}\")\n",
    "\n",
    "print(f\"\\nüí° Key Takeaways:\")\n",
    "print(f\"  ‚Ä¢ Always add delays between requests\")\n",
    "print(f\"  ‚Ä¢ Handle pagination systematically\")\n",
    "print(f\"  ‚Ä¢ Use advanced selectors for precise targeting\")\n",
    "print(f\"  ‚Ä¢ Monitor for anti-bot measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576b15f",
   "metadata": {},
   "source": [
    "## Chapter 4: APIs vs Web Scraping üîÑ\n",
    "\n",
    "**Before scraping, always check if an API exists!** APIs are usually faster, more reliable, and more respectful.\n",
    "\n",
    "### üÜö API vs Scraping Comparison\n",
    "\n",
    "| **Aspect** | **API** | **Web Scraping** |\n",
    "|------------|---------|------------------|\n",
    "| **Speed** | ‚ö° Very fast | üêå Slower (HTML parsing) |\n",
    "| **Reliability** | üéØ Stable structure | üîÑ Changes with website updates |\n",
    "| **Legal** | ‚úÖ Usually permitted | ‚öñÔ∏è Check terms of service |\n",
    "| **Rate Limits** | üìä Clearly defined | üö´ Risk of being blocked |\n",
    "| **Data Format** | üìã Structured (JSON/XML) | üï∏Ô∏è Unstructured (HTML) |\n",
    "| **Availability** | üé™ Not always available | üåê Most websites accessible |\n",
    "\n",
    "### üîç Finding APIs\n",
    "\n",
    "**How to discover if a site has an API:**\n",
    "1. **Check developer documentation** - Look for \"API\" or \"Developer\" pages\n",
    "2. **Inspect network traffic** - Use browser dev tools\n",
    "3. **Look for JSON responses** - Many sites use internal APIs\n",
    "4. **Search for \"[site name] API\"** - Google is your friend\n",
    "\n",
    "### üìä Working with JSON APIs\n",
    "\n",
    "**JSON (JavaScript Object Notation) is the standard API format:**\n",
    "```python\n",
    "# Typical API response structure\n",
    "{\n",
    "    \"status\": \"success\",\n",
    "    \"data\": [\n",
    "        {\"id\": 1, \"name\": \"Product A\", \"price\": 29.99},\n",
    "        {\"id\": 2, \"name\": \"Product B\", \"price\": 39.99}\n",
    "    ],\n",
    "    \"pagination\": {\n",
    "        \"page\": 1,\n",
    "        \"total_pages\": 10,\n",
    "        \"per_page\": 20\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec4b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó WORKING WITH APIs AND JSON\n",
      "================================\n",
      "üì° EXAMPLE: JSONPLACEHOLDER API\n",
      "===================================\n",
      "‚úÖ Successfully fetched 5 posts\n",
      "\n",
      "Post data structure:\n",
      "   userId  id                                              title  \\\n",
      "0       1   1  sunt aut facere repellat provident occaecati e...   \n",
      "1       1   2                                       qui est esse   \n",
      "2       1   3  ea molestias quasi exercitationem repellat qui...   \n",
      "3       1   4                               eum et est occaecati   \n",
      "4       1   5                                 nesciunt quas odio   \n",
      "\n",
      "                                                body  \n",
      "0  quia et suscipit\\nsuscipit recusandae consequu...  \n",
      "1  est rerum tempore vitae\\nsequi sint nihil repr...  \n",
      "2  et iusto sed quo iure\\nvoluptatem occaecati om...  \n",
      "3  ullam et saepe reiciendis voluptatem adipisci\\...  \n",
      "4  repudiandae veniam quaerat sunt sed\\nalias aut...  \n",
      "\n",
      "üìä MERGED DATA (Posts + User Info):\n",
      "                                               title           name  \\\n",
      "0  sunt aut facere repellat provident occaecati e...  Leanne Graham   \n",
      "1                                       qui est esse  Leanne Graham   \n",
      "2  ea molestias quasi exercitationem repellat qui...  Leanne Graham   \n",
      "3                               eum et est occaecati  Leanne Graham   \n",
      "4                                 nesciunt quas odio  Leanne Graham   \n",
      "\n",
      "               email  \n",
      "0  Sincere@april.biz  \n",
      "1  Sincere@april.biz  \n",
      "2  Sincere@april.biz  \n",
      "3  Sincere@april.biz  \n",
      "4  Sincere@april.biz  \n",
      "\n",
      "üÜö API VS SCRAPING COMPARISON DEMO\n",
      "========================================\n",
      "API approach:\n",
      "  ‚úÖ Fetched 10 items in 0.808 seconds\n",
      "  üìä Clean, structured JSON data\n",
      "  üéØ No HTML parsing required\n",
      "\n",
      "üìã DATA STRUCTURE COMPARISON:\n",
      "===================================\n",
      "API Response (JSON):\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n",
      "\n",
      "HTML data (requires parsing):\n",
      "\n",
      "<div class=\"post\">\n",
      "    <h2 class=\"title\">sunt aut facere repellat</h2>\n",
      "    <div class=\"body\">quia et suscipit...</div>\n",
      "    <span class=\"author\">Leanne Graham</span>\n",
      "    <span class=\"user-id\" data-id=\"1\">1</span>\n",
      "</div>\n",
      "\n",
      "\n",
      "üîß JSON DATA MANIPULATION\n",
      "==============================\n",
      "Nested JSON structure:\n",
      "{\n",
      "  \"user\": {\n",
      "    \"id\": 1,\n",
      "    \"profile\": {\n",
      "      \"name\": \"John Doe\",\n",
      "      \"contact\": {\n",
      "        \"email\": \"john@example.com\",\n",
      "        \"phone\": \"123-456-7890\"\n",
      "      }\n",
      "    },\n",
      "    \"posts\": [\n",
      "      {\n",
      "        \"id\": 1,\n",
      "        \"title\": \"First Post\",\n",
      "        \"likes\": 15\n",
      "      },\n",
      "      {\n",
      "        \"id\": 2,\n",
      "        \"title\": \"Second Post\",\n",
      "        \"likes\": 23\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Extracted information:\n",
      "  Name: John Doe\n",
      "  Email: john@example.com\n",
      "  Total likes: 38\n",
      "\n",
      "Flattened DataFrame:\n",
      "   user_id user_name        user_email  post_id   post_title  post_likes\n",
      "0        1  John Doe  john@example.com        1   First Post          15\n",
      "1        1  John Doe  john@example.com        2  Second Post          23\n",
      "\n",
      "üí° API BEST PRACTICES:\n",
      "  ‚Ä¢ Always check API documentation first\n",
      "  ‚Ä¢ Respect rate limits and use API keys\n",
      "  ‚Ä¢ Handle pagination in API responses\n",
      "  ‚Ä¢ Cache API responses when appropriate\n",
      "  ‚Ä¢ Fall back to scraping only when APIs aren't available\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Working with APIs and JSON data\n",
    "print(\"üîó WORKING WITH APIs AND JSON\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "def fetch_api_data(url, params=None):\n",
    "    \"\"\"\n",
    "    Fetch data from a JSON API with proper error handling\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if response is JSON\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if 'application/json' in content_type:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Response is not JSON: {content_type}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå API request failed: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid JSON response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Using a public API (JSONPlaceholder - a fake API for testing)\n",
    "print(\"üì° EXAMPLE: JSONPLACEHOLDER API\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Fetch sample posts from JSONPlaceholder\n",
    "posts_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "posts_data = fetch_api_data(posts_url, params={'_limit': 5})\n",
    "\n",
    "if posts_data:\n",
    "    print(f\"‚úÖ Successfully fetched {len(posts_data)} posts\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    print(f\"\\nPost data structure:\")\n",
    "    print(posts_df.head())\n",
    "    \n",
    "    # Fetch users data to join with posts\n",
    "    users_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "    users_data = fetch_api_data(users_url)\n",
    "    \n",
    "    if users_data:\n",
    "        users_df = pd.DataFrame(users_data)\n",
    "        \n",
    "        # Join posts with user information\n",
    "        merged_df = posts_df.merge(users_df[['id', 'name', 'email']], \n",
    "                                  left_on='userId', right_on='id', \n",
    "                                  suffixes=('_post', '_user'))\n",
    "        \n",
    "        print(f\"\\nüìä MERGED DATA (Posts + User Info):\")\n",
    "        print(merged_df[['title', 'name', 'email']].head())\n",
    "\n",
    "# Example: Comparing API vs Scraping approaches\n",
    "print(f\"\\nüÜö API VS SCRAPING COMPARISON DEMO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate API data extraction\n",
    "api_start_time = time.time()\n",
    "api_data = fetch_api_data(\"https://jsonplaceholder.typicode.com/posts\", {'_limit': 10})\n",
    "api_end_time = time.time()\n",
    "\n",
    "if api_data:\n",
    "    api_processing_time = api_end_time - api_start_time\n",
    "    print(f\"API approach:\")\n",
    "    print(f\"  ‚úÖ Fetched {len(api_data)} items in {api_processing_time:.3f} seconds\")\n",
    "    print(f\"  üìä Clean, structured JSON data\")\n",
    "    print(f\"  üéØ No HTML parsing required\")\n",
    "\n",
    "# Show the difference in data structure\n",
    "print(f\"\\nüìã DATA STRUCTURE COMPARISON:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# API data (clean JSON)\n",
    "if api_data:\n",
    "    sample_api_item = api_data[0]\n",
    "    print(\"API Response (JSON):\")\n",
    "    print(json.dumps(sample_api_item, indent=2))\n",
    "\n",
    "print(f\"\\nHTML data (requires parsing):\")\n",
    "sample_html_data = \"\"\"\n",
    "<div class=\"post\">\n",
    "    <h2 class=\"title\">sunt aut facere repellat</h2>\n",
    "    <div class=\"body\">quia et suscipit...</div>\n",
    "    <span class=\"author\">Leanne Graham</span>\n",
    "    <span class=\"user-id\" data-id=\"1\">1</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "print(sample_html_data)\n",
    "\n",
    "# Demonstrate JSON manipulation\n",
    "print(f\"\\nüîß JSON DATA MANIPULATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Working with nested JSON (common in APIs)\n",
    "nested_json = {\n",
    "    \"user\": {\n",
    "        \"id\": 1,\n",
    "        \"profile\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"contact\": {\n",
    "                \"email\": \"john@example.com\",\n",
    "                \"phone\": \"123-456-7890\"\n",
    "            }\n",
    "        },\n",
    "        \"posts\": [\n",
    "            {\"id\": 1, \"title\": \"First Post\", \"likes\": 15},\n",
    "            {\"id\": 2, \"title\": \"Second Post\", \"likes\": 23}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Nested JSON structure:\")\n",
    "print(json.dumps(nested_json, indent=2))\n",
    "\n",
    "# Extract data from nested JSON\n",
    "user_name = nested_json['user']['profile']['name']\n",
    "user_email = nested_json['user']['profile']['contact']['email']\n",
    "total_likes = sum(post['likes'] for post in nested_json['user']['posts'])\n",
    "\n",
    "print(f\"\\nExtracted information:\")\n",
    "print(f\"  Name: {user_name}\")\n",
    "print(f\"  Email: {user_email}\")\n",
    "print(f\"  Total likes: {total_likes}\")\n",
    "\n",
    "# Flatten nested JSON for DataFrame\n",
    "flattened_data = []\n",
    "for post in nested_json['user']['posts']:\n",
    "    flat_record = {\n",
    "        'user_id': nested_json['user']['id'],\n",
    "        'user_name': nested_json['user']['profile']['name'],\n",
    "        'user_email': nested_json['user']['profile']['contact']['email'],\n",
    "        'post_id': post['id'],\n",
    "        'post_title': post['title'],\n",
    "        'post_likes': post['likes']\n",
    "    }\n",
    "    flattened_data.append(flat_record)\n",
    "\n",
    "flat_df = pd.DataFrame(flattened_data)\n",
    "print(f\"\\nFlattened DataFrame:\")\n",
    "print(flat_df)\n",
    "\n",
    "print(f\"\\nüí° API BEST PRACTICES:\")\n",
    "print(f\"  ‚Ä¢ Always check API documentation first\")\n",
    "print(f\"  ‚Ä¢ Respect rate limits and use API keys\")\n",
    "print(f\"  ‚Ä¢ Handle pagination in API responses\")\n",
    "print(f\"  ‚Ä¢ Cache API responses when appropriate\")\n",
    "print(f\"  ‚Ä¢ Fall back to scraping only when APIs aren't available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd1bd1",
   "metadata": {},
   "source": [
    "## Chapter 5: Summary & Best Practices üéØ\n",
    "\n",
    "### üèÜ Key Takeaways\n",
    "\n",
    "**You've learned the fundamentals of web scraping!** Here's what we covered:\n",
    "\n",
    "1. **üîç HTML Structure** - Understanding the anatomy of web pages\n",
    "2. **üõ†Ô∏è Basic Scraping** - Using requests and BeautifulSoup\n",
    "3. **üöÄ Advanced Techniques** - Pagination, sessions, and error handling\n",
    "4. **üìä Data Processing** - Converting scraped data to DataFrames\n",
    "5. **üîå APIs vs Scraping** - Knowing when to use each approach\n",
    "\n",
    "### ‚öñÔ∏è Legal and Ethical Guidelines\n",
    "\n",
    "**üö® Always Remember:**\n",
    "- **Check robots.txt** - Respect website guidelines\n",
    "- **Read Terms of Service** - Understand legal restrictions\n",
    "- **Be respectful** - Don't overload servers with requests\n",
    "- **Add delays** - Space out your requests (1-2 seconds minimum)\n",
    "- **Use APIs when available** - They're designed for data access\n",
    "\n",
    "### üõ°Ô∏è Best Practices Checklist\n",
    "\n",
    "**Before You Scrape:**\n",
    "- [ ] Check if an API exists first\n",
    "- [ ] Read the website's robots.txt\n",
    "- [ ] Review terms of service\n",
    "- [ ] Test with small samples first\n",
    "\n",
    "**During Scraping:**\n",
    "- [ ] Use proper headers (User-Agent, etc.)\n",
    "- [ ] Implement rate limiting and delays\n",
    "- [ ] Handle errors gracefully\n",
    "- [ ] Monitor for blocking/captchas\n",
    "- [ ] Cache responses when appropriate\n",
    "\n",
    "**After Scraping:**\n",
    "- [ ] Clean and validate your data\n",
    "- [ ] Store data efficiently\n",
    "- [ ] Document your scraping process\n",
    "- [ ] Schedule updates responsibly\n",
    "\n",
    "### üîÆ Next Steps\n",
    "\n",
    "**To become a scraping expert, explore:**\n",
    "\n",
    "1. **Advanced Tools:**\n",
    "   - **Selenium/Playwright** - For JavaScript-heavy sites\n",
    "   - **Scrapy** - Professional scraping framework\n",
    "   - **Beautiful Soup alternatives** - lxml, html.parser\n",
    "\n",
    "2. **Handling Challenges:**\n",
    "   - **CAPTCHAs** - Detection and solving\n",
    "   - **IP blocking** - Proxy rotation\n",
    "   - **Dynamic content** - Browser automation\n",
    "   - **Anti-bot measures** - Stealth techniques\n",
    "\n",
    "3. **Data Pipeline:**\n",
    "   - **Storage** - Databases, files, cloud storage\n",
    "   - **Processing** - Data cleaning and validation\n",
    "   - **Monitoring** - Automated scraping jobs\n",
    "   - **Visualization** - Dashboards and reports\n",
    "\n",
    "### üìö Recommended Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Documentation](https://docs.python-requests.org/)\n",
    "- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "\n",
    "**Practice Sites:**\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/) - Beginner friendly\n",
    "- [Books to Scrape](http://books.toscrape.com/) - More complex structure\n",
    "- [Scrape This Site](https://scrapethissite.com/) - Various challenges\n",
    "\n",
    "**Remember:** Web scraping is a powerful tool for data collection, but with great power comes great responsibility! Always scrape ethically and respectfully. üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb16a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PRACTICAL EXERCISE\n",
      "=========================\n",
      "üìö BOOKS SCRAPING CHALLENGE\n",
      "Your mission: Scrape book data from http://books.toscrape.com/\n",
      "\n",
      "What to extract:\n",
      "  ‚Ä¢ Book titles\n",
      "  ‚Ä¢ Prices\n",
      "  ‚Ä¢ Star ratings\n",
      "  ‚Ä¢ Availability status\n",
      "\n",
      "üí° HINTS:\n",
      "  ‚Ä¢ Use inspector to examine the HTML structure\n",
      "  ‚Ä¢ Look for class names like 'product_pod'\n",
      "  ‚Ä¢ Ratings are in class names like 'star-rating Three'\n",
      "  ‚Ä¢ Prices are in <p class='price_color'>\n",
      "\n",
      "üöÄ BONUS CHALLENGES:\n",
      "  ‚Ä¢ Scrape multiple pages\n",
      "  ‚Ä¢ Calculate average price by rating\n",
      "  ‚Ä¢ Find the most expensive book\n",
      "  ‚Ä¢ Create a visualization of price distribution\n",
      "\n",
      "üìù CODE TEMPLATE:\n",
      "\n",
      "# Your solution here:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "# 1. Make request to books.toscrape.com\n",
      "# 2. Parse HTML with BeautifulSoup  \n",
      "# 3. Find book containers\n",
      "# 4. Extract title, price, rating, availability\n",
      "# 5. Convert to DataFrame\n",
      "# 6. Analyze the data\n",
      "\n",
      "# url = \"http://books.toscrape.com/\"\n",
      "# response = requests.get(url)\n",
      "# soup = BeautifulSoup(response.content, 'html.parser')\n",
      "# ... your code here ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "üíº REAL-WORLD SCRAPING WORKFLOW\n",
      "==================================================\n",
      "  1. üéØ Define your objective - What data do you need?\n",
      "  2. üîç Explore the website - Understand the structure\n",
      "  3. üìã Check legal requirements - robots.txt, ToS\n",
      "  4. üõ†Ô∏è Choose your tools - requests + BeautifulSoup or alternatives\n",
      "  5. üß™ Start small - Test with single pages first\n",
      "  6. üîÑ Scale up - Handle pagination and multiple pages\n",
      "  7. üßπ Clean data - Remove duplicates, handle missing values\n",
      "  8. üíæ Store results - Database, CSV, or other formats\n",
      "  9. üîÑ Automate - Schedule regular scraping if needed\n",
      "  10. üìä Analyze - Turn data into insights!\n",
      "\n",
      "üéâ CONGRATULATIONS!\n",
      "You now have the foundation to scrape data from the web!\n",
      "Remember: Practice makes perfect, and always scrape responsibly! üåü\n"
     ]
    }
   ],
   "source": [
    "# üéØ PRACTICAL EXERCISE: Your Turn to Scrape!\n",
    "print(\"üéØ PRACTICAL EXERCISE\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Exercise: Scrape book information from books.toscrape.com\n",
    "# This is a more complex scraping challenge!\n",
    "\n",
    "def scrape_books_challenge():\n",
    "    \"\"\"\n",
    "    Challenge: Scrape book data from books.toscrape.com\n",
    "    Try to extract: title, price, rating, availability\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìö BOOKS SCRAPING CHALLENGE\")\n",
    "    print(\"Your mission: Scrape book data from http://books.toscrape.com/\")\n",
    "    print(\"\\nWhat to extract:\")\n",
    "    print(\"  ‚Ä¢ Book titles\")\n",
    "    print(\"  ‚Ä¢ Prices\") \n",
    "    print(\"  ‚Ä¢ Star ratings\")\n",
    "    print(\"  ‚Ä¢ Availability status\")\n",
    "    \n",
    "    print(\"\\nüí° HINTS:\")\n",
    "    print(\"  ‚Ä¢ Use inspector to examine the HTML structure\")\n",
    "    print(\"  ‚Ä¢ Look for class names like 'product_pod'\")\n",
    "    print(\"  ‚Ä¢ Ratings are in class names like 'star-rating Three'\")\n",
    "    print(\"  ‚Ä¢ Prices are in <p class='price_color'>\")\n",
    "    \n",
    "    print(\"\\nüöÄ BONUS CHALLENGES:\")\n",
    "    print(\"  ‚Ä¢ Scrape multiple pages\")\n",
    "    print(\"  ‚Ä¢ Calculate average price by rating\")\n",
    "    print(\"  ‚Ä¢ Find the most expensive book\")\n",
    "    print(\"  ‚Ä¢ Create a visualization of price distribution\")\n",
    "    \n",
    "    print(\"\\nüìù CODE TEMPLATE:\")\n",
    "    template_code = '''\n",
    "# Your solution here:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Make request to books.toscrape.com\n",
    "# 2. Parse HTML with BeautifulSoup  \n",
    "# 3. Find book containers\n",
    "# 4. Extract title, price, rating, availability\n",
    "# 5. Convert to DataFrame\n",
    "# 6. Analyze the data\n",
    "\n",
    "# url = \"http://books.toscrape.com/\"\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# ... your code here ...\n",
    "'''\n",
    "    print(template_code)\n",
    "\n",
    "# Run the challenge setup\n",
    "scrape_books_challenge()\n",
    "\n",
    "# Example solution (commented out - let students try first!)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üíº REAL-WORLD SCRAPING WORKFLOW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "workflow_steps = [\n",
    "    \"1. üéØ Define your objective - What data do you need?\",\n",
    "    \"2. üîç Explore the website - Understand the structure\", \n",
    "    \"3. üìã Check legal requirements - robots.txt, ToS\",\n",
    "    \"4. üõ†Ô∏è Choose your tools - requests + BeautifulSoup or alternatives\",\n",
    "    \"5. üß™ Start small - Test with single pages first\",\n",
    "    \"6. üîÑ Scale up - Handle pagination and multiple pages\", \n",
    "    \"7. üßπ Clean data - Remove duplicates, handle missing values\",\n",
    "    \"8. üíæ Store results - Database, CSV, or other formats\",\n",
    "    \"9. üîÑ Automate - Schedule regular scraping if needed\",\n",
    "    \"10. üìä Analyze - Turn data into insights!\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(f\"\\nüéâ CONGRATULATIONS!\")\n",
    "print(\"You now have the foundation to scrape data from the web!\")\n",
    "print(\"Remember: Practice makes perfect, and always scrape responsibly! üåü\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
