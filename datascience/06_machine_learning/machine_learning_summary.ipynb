{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9238e75b",
   "metadata": {},
   "source": [
    "<!-- filepath: /home/marco/Marco/study_ml/datascience/06_machine_learning/machine_learning_summary.ipynb -->\n",
    "# Machine Learning Summary: Concepts, Code, and Visualizations\n",
    "\n",
    "This instructional notebook walks through core machine learning concepts covered in the 06_machine_learning module. It combines explanations, code, and visualizations with practical use cases.\n",
    "\n",
    "## Objectives\n",
    "- Understand regression (simple/multiple), classification (KNN, Decision Trees, Logistic Regression, SVM), and clustering (K-Means)\n",
    "- Learn multi-class strategies (Softmax, One-vs-All, One-vs-One)\n",
    "- Apply essential preprocessing (scaling, encoding)\n",
    "- Evaluate models with appropriate metrics\n",
    "- Explore regularization and hyperparameter tuning\n",
    "- Connect concepts to real-world applications\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Utilities\n",
    "2. Data Preprocessing (Scaling, One-Hot Encoding)\n",
    "3. Linear Regression (Simple & Multiple)\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "5. Decision Trees (Classification)\n",
    "6. Regression Trees\n",
    "7. Logistic Regression\n",
    "8. Support Vector Machines (SVM) & Kernels\n",
    "9. Multi-class: Softmax, One-vs-All, One-vs-One\n",
    "10. Clustering with K-Means + Customer Segmentation\n",
    "11. Model Evaluation Metrics\n",
    "12. Regularization (L1/L2) & Effects\n",
    "13. Hyperparameter Tuning (Grid Search)\n",
    "14. Ensemble Snapshot (Bagging/Random Forest)\n",
    "15. Best Practices & Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, normalize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    f1_score, jaccard_score, log_loss\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9acf1",
   "metadata": {},
   "source": [
    "## 1) Data Preprocessing\n",
    "\n",
    "Proper preprocessing ensures fair comparisons between models and stable performance.\n",
    "\n",
    "- Feature scaling: Standardization (mean 0, std 1) or Min-Max scaling (0-1)\n",
    "- One-hot encoding: Convert categorical features to indicator columns\n",
    "- Train/test split: Hold out unseen data for unbiased evaluation\n",
    "- Normalization: Row-wise normalization (e.g., L1) for some models\n",
    "- Handle missing values and outliers appropriately\n",
    "\n",
    "We will generate a small synthetic dataset to demonstrate scaling and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing demo: scaling and one-hot encoding on a mixed-type dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a small mixed dataset\n",
    "X_num, y_bin = make_classification(n_samples=400, n_features=4, n_informative=3, n_redundant=1, random_state=42)\n",
    "# Add a simple categorical column with 3 categories\n",
    "cats = np.random.choice([\"A\", \"B\", \"C\"], size=X_num.shape[0])\n",
    "X_df = pd.DataFrame(X_num, columns=[\"feat1\",\"feat2\",\"feat3\",\"feat4\"]).assign(cat=cats)\n",
    "\n",
    "num_features = [\"feat1\",\"feat2\",\"feat3\",\"feat4\"]\n",
    "cat_features = [\"cat\"]\n",
    "\n",
    "# OneHotEncoder compatibility for different sklearn versions\n",
    "try:\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_features),\n",
    "    (\"cat\", ohe, cat_features)\n",
    "])\n",
    "\n",
    "X_proc = preprocess.fit_transform(X_df)\n",
    "print(\"Original shape:\", X_df.shape, \"/ Processed shape:\", X_proc.shape)\n",
    "\n",
    "# Visualize distributions before/after scaling for one feature\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
    "ax[0].hist(X_df[\"feat1\"], bins=30, color=\"#88c\")\n",
    "ax[0].set_title(\"feat1 (raw)\")\n",
    "ax[1].hist(X_proc[:,0], bins=30, color=\"#c88\")\n",
    "ax[1].set_title(\"feat1 (standardized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61533683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Confusion matrix plotter\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516f896",
   "metadata": {},
   "source": [
    "## 2) Linear Regression (Simple & Multiple)\n",
    "\n",
    "Linear Regression models the relationship between a continuous target y and one or more features X.\n",
    "\n",
    "- Simple Linear Regression: y = θ₀ + θ₁ x₁\n",
    "- Multiple Linear Regression: y = θ₀ + Σ θⱼ xⱼ\n",
    "\n",
    "Key points:\n",
    "- Parameters estimated by Ordinary Least Squares (minimize sum of squared residuals).\n",
    "- Assumptions: linearity, independence, homoscedasticity, normality of residuals.\n",
    "- Evaluation: MAE, MSE, RMSE, R².\n",
    "\n",
    "We will fit simple and multiple linear regression, visualize residuals, and report metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression: simple & multiple on synthetic data\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=400, n_features=5, n_informative=4, noise=15.0, random_state=42)\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(Xr_train, yr_train)\n",
    "yr_pred = lin_reg.predict(Xr_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(yr_test, yr_pred)\n",
    "mse = mean_squared_error(yr_test, yr_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(yr_test, yr_pred)\n",
    "print({\"MAE\": round(mae,2), \"MSE\": round(mse,2), \"RMSE\": round(rmse,2), \"R2\": round(r2,3)})\n",
    "\n",
    "# Visualization: True vs Predicted and Residuals\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "ax[0].scatter(yr_test, yr_pred, alpha=0.6)\n",
    "ax[0].plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()], 'r--')\n",
    "ax[0].set_title('True vs Predicted')\n",
    "ax[0].set_xlabel('True y')\n",
    "ax[0].set_ylabel('Predicted y')\n",
    "\n",
    "residuals = yr_test - yr_pred\n",
    "ax[1].hist(residuals, bins=30, color=\"#6aa\")\n",
    "ax[1].set_title('Residuals distribution')\n",
    "plt.show()\n",
    "\n",
    "# Coefficients insight\n",
    "plt.bar(range(X_reg.shape[1]), lin_reg.coef_)\n",
    "plt.title('Linear Regression Coefficients')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9ae86",
   "metadata": {},
   "source": [
    "## 3) K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN classifies a sample based on the majority label among its k closest training points in feature space.\n",
    "\n",
    "Key ideas:\n",
    "- Distance metric (Euclidean by default) defines “closeness”.\n",
    "- Scaling features is critical because KNN is distance-based.\n",
    "- k controls bias-variance: small k can overfit; large k can underfit.\n",
    "\n",
    "We will visualize decision boundaries in 2D and plot accuracy vs k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN: decision boundary and accuracy vs k\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_knn, y_knn = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                   n_clusters_per_class=1, class_sep=1.2, random_state=42)\n",
    "Xk_train, Xk_test, yk_train, yk_test = train_test_split(X_knn, y_knn, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler_knn = StandardScaler().fit(Xk_train)\n",
    "Xk_train_s = scaler_knn.transform(Xk_train)\n",
    "Xk_test_s = scaler_knn.transform(Xk_test)\n",
    "\n",
    "# Accuracy vs k\n",
    "k_values = list(range(1, 21))\n",
    "accs = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(Xk_train_s, yk_train)\n",
    "    accs.append(accuracy_score(yk_test, knn.predict(Xk_test_s)))\n",
    "\n",
    "plt.plot(k_values, accs, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Accuracy vs k')\n",
    "plt.show()\n",
    "\n",
    "# Decision boundary for k=5\n",
    "k_best = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=k_best).fit(Xk_train_s, yk_train)\n",
    "\n",
    "x_min, x_max = Xk_train_s[:, 0].min() - 1, Xk_train_s[:, 0].max() + 1\n",
    "y_min, y_max = Xk_train_s[:, 1].min() - 1, Xk_train_s[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "cmap_light = ListedColormap(['#FFBBBB', '#BBFFBB'])\n",
    "cmap_bold = ['#FF0000', '#00AA00']\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.7)\n",
    "plt.scatter(Xk_train_s[:, 0], Xk_train_s[:, 1], c=yk_train, cmap=ListedColormap(cmap_bold), edgecolor='k', s=25)\n",
    "plt.title(f'KNN Decision Boundary (k={k_best})')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred_knn = knn.predict(Xk_test_s)\n",
    "cm = confusion_matrix(yk_test, y_pred_knn)\n",
    "plot_confusion_matrix(cm, classes=['Class 0','Class 1'], normalize=False, title='KNN Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f55fa",
   "metadata": {},
   "source": [
    "## 4) Decision Trees (Classification)\n",
    "\n",
    "Decision Trees split the feature space into regions using if-else rules.\n",
    "\n",
    "- Nodes: decision points on features; Leaves: final class\n",
    "- Splitting criteria: Gini impurity or Entropy (information gain)\n",
    "- Control overfitting via max_depth, min_samples_split, min_samples_leaf\n",
    "- Pros: interpretable, handles non-linear boundaries; Cons: can overfit\n",
    "\n",
    "We will train a tree, visualize its decision boundary, and review metrics and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree: classification with decision boundary and metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_dt, y_dt = make_classification(n_samples=600, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                 n_clusters_per_class=1, class_sep=1.2, random_state=7)\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(X_dt, y_dt, test_size=0.3, random_state=7)\n",
    "\n",
    "clf_dt = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=7)\n",
    "clf_dt.fit(Xd_train, yd_train)\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_dt[:, 0].min() - 1, X_dt[:, 0].max() + 1\n",
    "y_min, y_max = X_dt[:, 1].min() - 1, X_dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "Z = clf_dt.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "cmap_light = ListedColormap(['#FFEEEE', '#EEFFEE'])\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.7)\n",
    "plt.scatter(Xd_train[:, 0], Xd_train[:, 1], c=yd_train, cmap=ListedColormap(['#FF0000','#00AA00']), edgecolor='k', s=20)\n",
    "plt.title('Decision Tree Decision Boundary (depth=4)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "yd_pred = clf_dt.predict(Xd_test)\n",
    "print(classification_report(yd_test, yd_pred))\n",
    "cm = confusion_matrix(yd_test, yd_pred)\n",
    "plot_confusion_matrix(cm, classes=['Class 0','Class 1'], normalize=True, title='Decision Tree (Normalized CM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a2e21",
   "metadata": {},
   "source": [
    "## 5) Regression Trees\n",
    "\n",
    "Decision Trees can also predict continuous targets (regression trees).\n",
    "\n",
    "- Split criterion: minimize variance (MSE/MAE) within nodes\n",
    "- Leaves output continuous values (e.g., mean of training targets in leaf)\n",
    "- Evaluation: MAE, MSE, RMSE, R²\n",
    "\n",
    "We will train a regression tree, compare to linear regression, and visualize predictions vs truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff891d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Trees vs Linear Regression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg2, y_reg2 = make_regression(n_samples=600, n_features=4, n_informative=3, noise=20.0, random_state=0)\n",
    "Xrt_train, Xrt_test, yrt_train, yrt_test = train_test_split(X_reg2, y_reg2, test_size=0.3, random_state=0)\n",
    "\n",
    "# Linear Regression baseline\n",
    "lin = LinearRegression().fit(Xrt_train, yrt_train)\n",
    "y_pred_lin = lin.predict(Xrt_test)\n",
    "\n",
    "# Regression Tree\n",
    "rt = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "rt.fit(Xrt_train, yrt_train)\n",
    "y_pred_rt = rt.predict(Xrt_test)\n",
    "\n",
    "# Metrics\n",
    "metrics_lin = {\n",
    "    'MAE': mean_absolute_error(yrt_test, y_pred_lin),\n",
    "    'MSE': mean_squared_error(yrt_test, y_pred_lin),\n",
    "    'RMSE': np.sqrt(mean_squared_error(yrt_test, y_pred_lin)),\n",
    "    'R2': r2_score(yrt_test, y_pred_lin)\n",
    "}\n",
    "metrics_rt = {\n",
    "    'MAE': mean_absolute_error(yrt_test, y_pred_rt),\n",
    "    'MSE': mean_squared_error(yrt_test, y_pred_rt),\n",
    "    'RMSE': np.sqrt(mean_squared_error(yrt_test, y_pred_rt)),\n",
    "    'R2': r2_score(yrt_test, y_pred_rt)\n",
    "}\n",
    "print('Linear Regression:', {k: round(v,3) for k,v in metrics_lin.items()})\n",
    "print('Regression Tree   :', {k: round(v,3) for k,v in metrics_rt.items()})\n",
    "\n",
    "# Plot predictions vs truth\n",
    "plt.scatter(yrt_test, y_pred_lin, alpha=0.5, label='Linear')\n",
    "plt.scatter(yrt_test, y_pred_rt, alpha=0.5, label='Tree')\n",
    "plt.plot([yrt_test.min(), yrt_test.max()], [yrt_test.min(), yrt_test.max()], 'k--')\n",
    "plt.legend(); plt.title('Regression: Predictions vs Truth'); plt.xlabel('True'); plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897fdf5",
   "metadata": {},
   "source": [
    "## 6) Logistic Regression\n",
    "\n",
    "Logistic Regression models the probability of class membership using the logistic (sigmoid) function.\n",
    "\n",
    "- Sigmoid: σ(z) = 1 / (1 + e^{-z}) with z = θᵀx\n",
    "- Outputs probabilities; threshold (e.g., 0.5) maps to class labels\n",
    "- Trained via maximum likelihood with regularization (L2 by default)\n",
    "- Metrics: Accuracy, Precision/Recall/F1, Jaccard, Log Loss\n",
    "\n",
    "We will train a logistic model, visualize the decision boundary, and evaluate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: decision boundary and metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_lr, y_lr = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                 n_clusters_per_class=1, class_sep=1.2, random_state=12)\n",
    "Xl_train, Xl_test, yl_train, yl_test = train_test_split(X_lr, y_lr, test_size=0.3, random_state=12)\n",
    "\n",
    "scaler_lr = StandardScaler().fit(Xl_train)\n",
    "Xl_train_s = scaler_lr.transform(Xl_train)\n",
    "Xl_test_s = scaler_lr.transform(Xl_test)\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(Xl_train_s, yl_train)\n",
    "\n",
    "# Decision boundary\n",
    "grid_x, grid_y = np.meshgrid(\n",
    "    np.linspace(Xl_train_s[:,0].min()-1, Xl_train_s[:,0].max()+1, 300),\n",
    "    np.linspace(Xl_train_s[:,1].min()-1, Xl_train_s[:,1].max()+1, 300)\n",
    ")\n",
    "Z = log_reg.predict(np.c_[grid_x.ravel(), grid_y.ravel()]).reshape(grid_x.shape)\n",
    "plt.contourf(grid_x, grid_y, Z, cmap=ListedColormap(['#FEE','#EEF']), alpha=0.7)\n",
    "plt.scatter(Xl_train_s[:,0], Xl_train_s[:,1], c=yl_train, cmap=ListedColormap(['#F00','#00A']), edgecolor='k', s=20)\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.show()\n",
    "\n",
    "yl_pred = log_reg.predict(Xl_test_s)\n",
    "yl_proba = log_reg.predict_proba(Xl_test_s)\n",
    "print(classification_report(yl_test, yl_pred))\n",
    "print('Log Loss:', round(log_loss(yl_test, yl_proba), 4))\n",
    "plot_confusion_matrix(confusion_matrix(yl_test, yl_pred), ['0','1'], True, 'Logistic Regression (Normalized CM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d63e6",
   "metadata": {},
   "source": [
    "## 7) Support Vector Machines (SVM) & Kernels\n",
    "\n",
    "SVM finds the hyperplane that maximizes the margin between classes.\n",
    "\n",
    "- Support vectors: training samples on the margin\n",
    "- Margin: distance between class boundaries\n",
    "- C (regularization):\n",
    "  - Large C: hard margin (low bias, high variance)\n",
    "  - Small C: soft margin (higher bias, lower variance)\n",
    "- Kernels:\n",
    "  - Linear: for linearly separable data\n",
    "  - RBF: non-linear boundaries; controlled by gamma\n",
    "  - Polynomial, Sigmoid (less common)\n",
    "- gamma (RBF):\n",
    "  - Small gamma: smoother boundary (underfit)\n",
    "  - Large gamma: wiggly boundary (overfit)\n",
    "\n",
    "We will compare linear vs RBF kernels and visualize decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: compare linear vs RBF kernels\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_svm, y_svm = make_classification(n_samples=600, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                   n_clusters_per_class=1, class_sep=1.2, random_state=21)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_svm, y_svm, test_size=0.3, random_state=21)\n",
    "scaler_svm = StandardScaler().fit(Xs_train)\n",
    "Xs_train_s = scaler_svm.transform(Xs_train)\n",
    "Xs_test_s = scaler_svm.transform(Xs_test)\n",
    "\n",
    "models = {\n",
    "    'linear': SVC(kernel='linear', C=1.0, probability=True, random_state=21),\n",
    "    'rbf_low_gamma': SVC(kernel='rbf', C=1.0, gamma=0.5, probability=True, random_state=21),\n",
    "    'rbf_high_gamma': SVC(kernel='rbf', C=1.0, gamma=5.0, probability=True, random_state=21)\n",
    "}\n",
    "\n",
    "for name, mdl in models.items():\n",
    "    mdl.fit(Xs_train_s, ys_train)\n",
    "    y_pred = mdl.predict(Xs_test_s)\n",
    "    acc = accuracy_score(ys_test, y_pred)\n",
    "    print(f\"{name}: accuracy = {acc:.3f}\")\n",
    "\n",
    "    # Decision boundary\n",
    "    x_min, x_max = Xs_train_s[:, 0].min() - 1, Xs_train_s[:, 0].max() + 1\n",
    "    y_min, y_max = Xs_train_s[:, 1].min() - 1, Xs_train_s[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    Z = mdl.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFECEC','#ECFFEC']), alpha=0.7)\n",
    "    plt.scatter(Xs_train_s[:, 0], Xs_train_s[:, 1], c=ys_train, cmap=ListedColormap(['#FF0000','#00AA00']), edgecolor='k', s=20)\n",
    "    plt.title(f'SVM Decision Boundary: {name}')\n",
    "    plt.xlabel('Feature 1 (scaled)')\n",
    "    plt.ylabel('Feature 2 (scaled)')\n",
    "    plt.show()\n",
    "\n",
    "    cm = confusion_matrix(ys_test, y_pred)\n",
    "    plot_confusion_matrix(cm, classes=['Class 0','Class 1'], normalize=True, title=f'{name} (Normalized CM)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
