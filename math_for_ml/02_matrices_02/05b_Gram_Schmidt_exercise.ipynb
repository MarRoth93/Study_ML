{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process: From Linearly Independent to Orthonormal\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "- Implement the Gram-Schmidt orthogonalization process step by step\n",
    "- Transform any set of linearly independent vectors into an orthonormal basis\n",
    "- Calculate the dimension of a vector space spanned by a given set of vectors\n",
    "- Understand the geometric intuition behind orthogonalization\n",
    "\n",
    "## 📖 Mathematical Background\n",
    "\n",
    "The **Gram-Schmidt process** is a fundamental algorithm in linear algebra that converts a set of linearly independent vectors into an **orthonormal basis** for the same vector space. This process is crucial in many applications including:\n",
    "\n",
    "- **QR decomposition** of matrices\n",
    "- **Principal Component Analysis (PCA)**\n",
    "- **Least squares** regression\n",
    "- **Signal processing** and **data compression**\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Orthogonal vectors**: Two vectors $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal if their dot product is zero: $\\mathbf{u} \\cdot \\mathbf{v} = 0$\n",
    "\n",
    "**Orthonormal vectors**: Vectors that are both orthogonal to each other and have unit length (norm = 1)\n",
    "\n",
    "**Vector projection**: The projection of vector $\\mathbf{a}$ onto vector $\\mathbf{b}$ is:\n",
    "$$\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{b} \\cdot \\mathbf{b}}\\mathbf{b}$$\n",
    "\n",
    "### The Algorithm\n",
    "Given vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$, the Gram-Schmidt process produces orthonormal vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n$:\n",
    "\n",
    "1. **First vector**: $\\mathbf{u}_1 = \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|}$\n",
    "\n",
    "2. **Subsequent vectors**: For $k = 2, 3, \\ldots, n$:\n",
    "   - Remove projections: $\\mathbf{w}_k = \\mathbf{v}_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\mathbf{u}_j}\\mathbf{v}_k$\n",
    "   - Normalize: $\\mathbf{u}_k = \\frac{\\mathbf{w}_k}{\\|\\mathbf{w}_k\\|}$ (if $\\mathbf{w}_k \\neq \\mathbf{0}$)\n",
    "\n",
    "## 🎯 Your Mission\n",
    "In this assignment you will write a function to perform the Gram-Schmidt procedure, which takes a list of vectors and forms an orthonormal basis from this set.\n",
    "As a corollary, the procedure allows us to determine the dimension of the space spanned by the basis vectors, which is equal to or less than the space which the vectors sit.\n",
    "\n",
    "You'll start by completing a function for 4 basis vectors, before generalising to when an arbitrary number of vectors are given.\n",
    "\n",
    "Again, a framework for the function has already been written.\n",
    "Look through the code, and you'll be instructed where to make changes.\n",
    "We'll do the first two rows, and you can use this as a guide to do the last two.\n",
    "\n",
    "### Matrices in Python\n",
    "Remember the structure for matrices in *numpy* is:\n",
    "```python\n",
    "A[0, 0]  A[0, 1]  A[0, 2]  A[0, 3]\n",
    "A[1, 0]  A[1, 1]  A[1, 2]  A[1, 3]\n",
    "A[2, 0]  A[2, 1]  A[2, 2]  A[2, 3]\n",
    "A[3, 0]  A[3, 1]  A[3, 2]  A[3, 3]\n",
    "```\n",
    "\n",
    "**Key Operations You'll Need:**\n",
    "- Access individual elements: `A[n, m]`\n",
    "- Access whole rows: `A[n]` or `A[n, :]`\n",
    "- **Access whole columns**: `A[:, m]` ← *This selects the m'th column*\n",
    "- **Dot product**: Use `u @ v` to compute $\\mathbf{u} \\cdot \\mathbf{v}$\n",
    "- **Vector norm**: Use `la.norm(v)` to compute $\\|\\mathbf{v}\\|$\n",
    "\n",
    "### 💡 Implementation Tips\n",
    "1. **Work column by column**: Each column represents a vector to orthogonalize\n",
    "2. **Subtract projections**: Remove components parallel to previous orthonormal vectors\n",
    "3. **Check for linear dependence**: If a vector becomes very small after projection removal, it's linearly dependent\n",
    "4. **Normalize carefully**: Only normalize non-zero vectors\n",
    "\n",
    "All the code you should complete will be at the same level of indentation as the instruction comment.\n",
    "\n",
    "### How to submit\n",
    "Edit the code in the cell below to complete the assignment.\n",
    "Once you are finished and happy with it, press the *Submit Assignment* button at the top of this notebook.\n",
    "\n",
    "Please don't change any of the function names, as these will be checked by the grading script.\n",
    "\n",
    "If you have further questions about submissions or programming assignments, here is a [list](https://www.coursera.org/learn/linear-algebra-machine-learning/discussions/weeks/1/threads/jB4klkn5EeibtBIQyzFmQg) of Q&A. You can also raise an issue on the discussion forum. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 88 (611512097.py, line 100)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn B\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 88\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "verySmallNumber = 1e-14 # That's 1×10⁻¹⁴ = 0.00000000000001\n",
    "\n",
    "# Our first function will perform the Gram-Schmidt procedure for 4 basis vectors.\n",
    "# We'll take this list of vectors as the columns of a matrix, A.\n",
    "# We'll then go through the vectors one at a time and set them to be orthogonal\n",
    "# to all the vectors that came before it. Before normalising.\n",
    "# Follow the instructions inside the function at each comment.\n",
    "# You will be told where to add code to complete the function.\n",
    "def gsBasis4(A) :\n",
    "    \"\"\"\n",
    "    Perform Gram-Schmidt orthogonalization on exactly 4 vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    A : numpy array of shape (n, 4)\n",
    "        Matrix where each column is a vector to be orthogonalized\n",
    "        \n",
    "    Returns:\n",
    "    B : numpy array of shape (n, 4)\n",
    "        Matrix where each column is an orthonormal vector\n",
    "    \"\"\"\n",
    "    B = np.array(A, dtype=np.float_) # Make B as a copy of A, since we're going to alter it's values.\n",
    "    \n",
    "    # STEP 1: Handle the first vector (column 0)\n",
    "    # The zeroth column is easy, since it has no other vectors to make it normal to.\n",
    "    # All that needs to be done is to normalise it. I.e. divide by its modulus, or norm.\n",
    "    B[:, 0] = B[:, 0] / la.norm(B[:, 0])\n",
    "    \n",
    "    # STEP 2: Handle the second vector (column 1)\n",
    "    # For the first column, we need to subtract any overlap with our new zeroth vector.\n",
    "    # This removes the component of v₁ that's parallel to u₀\n",
    "    B[:, 1] = B[:, 1] - B[:, 1] @ B[:, 0] * B[:, 0]\n",
    "    # If there's anything left after that subtraction, then B[:, 1] is linearly independant of B[:, 0]\n",
    "    # If this is the case, we can normalise it. Otherwise we'll set that vector to zero.\n",
    "    if la.norm(B[:, 1]) > verySmallNumber :\n",
    "        B[:, 1] = B[:, 1] / la.norm(B[:, 1])\n",
    "    else :\n",
    "        B[:, 1] = np.zeros_like(B[:, 1])\n",
    "    \n",
    "    # STEP 3: Handle the third vector (column 2)\n",
    "    # Now we need to repeat the process for column 2.\n",
    "    # Insert two lines of code, the first to subtract the overlap with the zeroth vector,\n",
    "    # and the second to subtract the overlap with the first.\n",
    "    # HINT: B[:, 2] = B[:, 2] - (B[:, 2] @ B[:, ?]) * B[:, ?]\n",
    "    \n",
    "    \n",
    "    # Again we'll need to normalise our new vector.\n",
    "    # Copy and adapt the normalisation fragment from above to column 2.\n",
    "    # HINT: Check if la.norm(B[:, 2]) > verySmallNumber, then normalize or set to zero\n",
    "\n",
    "    \n",
    "    # STEP 4: Handle the fourth vector (column 3)\n",
    "    # Finally, column three:\n",
    "    # Insert code to subtract the overlap with the first three vectors.\n",
    "    # HINT: You need three lines, one for each previous orthonormal vector\n",
    "\n",
    "    \n",
    "    # Now normalise if possible\n",
    "    # HINT: Same pattern as above - check norm, then normalize or zero out\n",
    "   \n",
    "    \n",
    "    # Finally, we return the result:\n",
    "    return B\n",
    "\n",
    "# The second part of this exercise will generalise the procedure.\n",
    "# Previously, we could only have four vectors, and there was a lot of repeating in the code.\n",
    "# We'll use a for-loop here to iterate the process for each vector.\n",
    "def gsBasis(A) :\n",
    "    \"\"\"\n",
    "    Perform Gram-Schmidt orthogonalization on any number of vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    A : numpy array of shape (n, m)\n",
    "        Matrix where each column is a vector to be orthogonalized\n",
    "        \n",
    "    Returns:\n",
    "    B : numpy array of shape (n, m)\n",
    "        Matrix where each column is an orthonormal vector\n",
    "    \"\"\"\n",
    "    B = np.array(A, dtype=np.float_) # Make B as a copy of A, since we're going to alter it's values.\n",
    "    \n",
    "    # Loop over all vectors, starting with zero, label them with i\n",
    "    for i in range(B.shape[1]) :\n",
    "        # Inside that loop, loop over all previous vectors, j, to subtract.\n",
    "        for j in range(i) :\n",
    "            # Complete the code to subtract the overlap with previous vectors.\n",
    "            # you'll need the current vector B[:, i] and a previous vector B[:, j]\n",
    "            # HINT: B[:, i] = B[:, i] - ...what goes here?...\n",
    "\n",
    "        # Next insert code to do the normalisation test for B[:, i]\n",
    "        # HINT: Same pattern as in gsBasis4 - check if norm > verySmallNumber\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "    # Finally, we return the result:\n",
    "    return B\n",
    "\n",
    "# This function uses the Gram-schmidt process to calculate the dimension\n",
    "# spanned by a list of vectors.\n",
    "# Since each vector is normalised to one, or is zero,\n",
    "# the sum of all the norms will be the dimension.\n",
    "def dimensions(A) :\n",
    "    \"\"\"\n",
    "    Calculate the dimension of the space spanned by the columns of A.\n",
    "    \n",
    "    Parameters:\n",
    "    A : numpy array\n",
    "        Matrix where each column is a vector\n",
    "        \n",
    "    Returns:\n",
    "    int : The dimension of the span of the columns\n",
    "    \"\"\"\n",
    "    return np.sum(la.norm(gsBasis(A), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test Your Implementation\n",
    "\n",
    "Before submitting, it's crucial to test your code thoroughly! Run the cell above (select it and press Shift+Enter) to implement your functions.\n",
    "\n",
    "The test cases below will help you verify that your implementation works correctly:\n",
    "\n",
    "1. **Basic functionality test**: Does your algorithm produce orthonormal vectors?\n",
    "2. **Idempotency test**: Running Gram-Schmidt on already orthonormal vectors should give the same result\n",
    "3. **Dimension detection**: Can your algorithm detect linear dependence?\n",
    "4. **Edge cases**: How does it handle non-square matrices and degenerate cases?\n",
    "\n",
    "### 🎯 What to Look For:\n",
    "- **Orthogonality**: Dot products between different output vectors should be ≈ 0\n",
    "- **Normalization**: Each output vector should have length ≈ 1 (unless it's the zero vector)\n",
    "- **Span preservation**: The output vectors should span the same space as the input vectors\n",
    "- **Linear dependence handling**: Linearly dependent vectors should become zero vectors\n",
    "\n",
    "Try out your code on the test cases below, and feel free to create your own tricky examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m|u1| = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mla.norm(u1)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, |u2| = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mla.norm(u2)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (unit length!)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Run the visualization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mvisualize_gram_schmidt_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mvisualize_gram_schmidt_2d\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Visualize Gram-Schmidt process in 2D for intuition\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Two linearly independent vectors in 2D\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m v1 = \u001b[43mnp\u001b[49m.array([\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m      9\u001b[39m v2 = np.array([\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m])\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Apply Gram-Schmidt manually for visualization\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's visualize the Gram-Schmidt process to build geometric intuition\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_gram_schmidt_2d():\n",
    "    \"\"\"Visualize Gram-Schmidt process in 2D for intuition\"\"\"\n",
    "    # Two linearly independent vectors in 2D\n",
    "    v1 = np.array([3, 1])\n",
    "    v2 = np.array([1, 2])\n",
    "    \n",
    "    # Apply Gram-Schmidt manually for visualization\n",
    "    u1 = v1 / la.norm(v1)  # Normalize first vector\n",
    "    \n",
    "    # Project v2 onto u1 and subtract to get orthogonal component\n",
    "    proj_v2_u1 = (v2 @ u1) * u1\n",
    "    w2 = v2 - proj_v2_u1\n",
    "    u2 = w2 / la.norm(w2)  # Normalize\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Original vectors\n",
    "    ax1.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='$v_1$')\n",
    "    ax1.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.005, label='$v_2$')\n",
    "    ax1.set_xlim(-0.5, 4)\n",
    "    ax1.set_ylim(-0.5, 3)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Original Vectors')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    \n",
    "    # Plot 2: Gram-Schmidt process visualization\n",
    "    ax2.quiver(0, 0, u1[0], u1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='$u_1$ (normalized $v_1$)')\n",
    "    ax2.quiver(0, 0, proj_v2_u1[0], proj_v2_u1[1], angles='xy', scale_units='xy', scale=1, color='orange', width=0.005, \n",
    "              linestyle='--', alpha=0.7, label='proj$_{u_1}v_2$')\n",
    "    ax2.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.003, \n",
    "              alpha=0.5, label='$v_2$ (original)')\n",
    "    ax2.quiver(0, 0, u2[0], u2[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.005, label='$u_2$ (orthogonalized)')\n",
    "    \n",
    "    # Show the orthogonal component\n",
    "    ax2.quiver(proj_v2_u1[0], proj_v2_u1[1], w2[0], w2[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='purple', width=0.003, alpha=0.8, label='$v_2 - $ proj$_{u_1}v_2$')\n",
    "    \n",
    "    ax2.set_xlim(-0.5, 4)\n",
    "    ax2.set_ylim(-0.5, 3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.set_title('Gram-Schmidt Process')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Verify orthogonality\n",
    "    print(f\"Original vectors v1 and v2:\")\n",
    "    print(f\"v1 = {v1}\")\n",
    "    print(f\"v2 = {v2}\")\n",
    "    print(f\"v1 · v2 = {v1 @ v2:.3f} (not orthogonal)\")\n",
    "    print(f\"|v1| = {la.norm(v1):.3f}, |v2| = {la.norm(v2):.3f} (not unit length)\")\n",
    "    print()\n",
    "    print(f\"After Gram-Schmidt u1 and u2:\")\n",
    "    print(f\"u1 = {u1}\")\n",
    "    print(f\"u2 = {u2}\")\n",
    "    print(f\"u1 · u2 = {u1 @ u2:.10f} (orthogonal!)\")\n",
    "    print(f\"|u1| = {la.norm(u1):.3f}, |u2| = {la.norm(u2):.3f} (unit length!)\")\n",
    "\n",
    "# Run the visualization\n",
    "visualize_gram_schmidt_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 1: Four linearly independent vectors in 4D space\n",
    "# These vectors are chosen to be clearly non-orthogonal initially\n",
    "print(\"🔍 Test Case 1: Four linearly independent vectors\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "V = np.array([[1, 0, 3, 5],\n",
    "              [0, 2, 1, 4], \n",
    "              [4, 1, 2, 3],\n",
    "              [2, -1, 4, 1]], dtype=np.float_)\n",
    "\n",
    "print(\"Original matrix V:\")\n",
    "print(V)\n",
    "print(f\"Shape: {V.shape}\")\n",
    "\n",
    "# Test your gsBasis4 function\n",
    "result = gsBasis4(V)\n",
    "print(\"\\nAfter Gram-Schmidt (gsBasis4):\")\n",
    "print(result)\n",
    "\n",
    "# Let's verify orthogonality and normalization\n",
    "print(\"\\n📊 Verification:\")\n",
    "print(\"Column norms:\", [f\"{la.norm(result[:, i]):.6f}\" for i in range(4)])\n",
    "\n",
    "# Check orthogonality by computing all pairwise dot products\n",
    "print(\"\\nDot product matrix (should be close to identity):\")\n",
    "dot_matrix = result.T @ result\n",
    "print(np.round(dot_matrix, 6))\n",
    "\n",
    "# Check if it's close to identity matrix\n",
    "is_orthonormal = np.allclose(dot_matrix, np.eye(4), atol=1e-10)\n",
    "print(f\"\\nIs result orthonormal? {is_orthonormal}\")\n",
    "print(f\"Maximum off-diagonal element: {np.max(np.abs(dot_matrix - np.eye(4))):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 2: Idempotency Test\n",
    "# Once you've done Gram-Schmidt once, doing it again should give you the same result\n",
    "print(\"🔄 Test Case 2: Idempotency (Running Gram-Schmidt twice)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "U = gsBasis4(V)\n",
    "U_again = gsBasis4(U)\n",
    "\n",
    "print(\"Original orthonormal matrix U:\")\n",
    "print(np.round(U, 6))\n",
    "\n",
    "print(\"\\nAfter applying Gram-Schmidt again:\")\n",
    "print(np.round(U_again, 6))\n",
    "\n",
    "# Check if they're the same\n",
    "difference = np.max(np.abs(U - U_again))\n",
    "print(f\"\\nMaximum difference between U and U_again: {difference:.2e}\")\n",
    "\n",
    "is_same = np.allclose(U, U_again, atol=1e-12)\n",
    "print(f\"Are they the same (within tolerance)? {is_same}\")\n",
    "\n",
    "if is_same:\n",
    "    print(\"✅ Great! Gram-Schmidt is idempotent on orthonormal matrices.\")\n",
    "else:\n",
    "    print(\"❌ There might be an issue with your implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 3: Compare gsBasis4 with general gsBasis function\n",
    "print(\"⚖️ Test Case 3: Comparing specific vs general implementation\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "result_specific = gsBasis4(V)\n",
    "result_general = gsBasis(V)\n",
    "\n",
    "print(\"Result from gsBasis4:\")\n",
    "print(np.round(result_specific, 6))\n",
    "\n",
    "print(\"\\nResult from gsBasis (general):\")\n",
    "print(np.round(result_general, 6))\n",
    "\n",
    "# Check if they produce the same result\n",
    "max_diff = np.max(np.abs(result_specific - result_general))\n",
    "print(f\"\\nMaximum difference between the two methods: {max_diff:.2e}\")\n",
    "\n",
    "are_same = np.allclose(result_specific, result_general, atol=1e-12)\n",
    "print(f\"Do both methods give the same result? {are_same}\")\n",
    "\n",
    "if are_same:\n",
    "    print(\"✅ Excellent! Your general implementation matches the specific one.\")\n",
    "else:\n",
    "    print(\"❌ The implementations differ. Check your general gsBasis function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23643312,  0.18771349,  0.22132104],\n",
       "       [ 0.15762208,  0.74769023, -0.64395812],\n",
       "       [ 0.15762208,  0.57790444,  0.72904263],\n",
       "       [ 0.94573249, -0.26786082, -0.06951101]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 4: Non-square matrices (More vectors than dimensions)\n",
    "print(\"📐 Test Case 4: Non-square matrix (4×3 - more rows than columns)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 4 rows, 3 columns - vectors in 4D space but only 3 vectors\n",
    "A = np.array([[2, 1, 4],\n",
    "              [1, 3, -2],\n",
    "              [3, 2, 1],\n",
    "              [1, 4, 5]], dtype=np.float_)\n",
    "\n",
    "print(\"Original matrix A (4×3):\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape}\")\n",
    "print(\"Each column is a vector in 4D space\")\n",
    "\n",
    "result_A = gsBasis(A)\n",
    "print(\"\\nAfter Gram-Schmidt:\")\n",
    "print(np.round(result_A, 6))\n",
    "\n",
    "# Verify orthonormality\n",
    "print(\"\\n📊 Analysis:\")\n",
    "dot_matrix_A = result_A.T @ result_A\n",
    "print(\"Gram matrix (A^T @ A):\")\n",
    "print(np.round(dot_matrix_A, 6))\n",
    "\n",
    "print(f\"\\nColumn norms: {[f'{la.norm(result_A[:, i]):.6f}' for i in range(A.shape[1])]}\")\n",
    "print(f\"Is orthonormal? {np.allclose(dot_matrix_A, np.eye(A.shape[1]), atol=1e-10)}\")\n",
    "\n",
    "# Check that we still span the same space\n",
    "original_rank = np.linalg.matrix_rank(A)\n",
    "result_rank = np.linalg.matrix_rank(result_A)\n",
    "print(f\"\\nOriginal matrix rank: {original_rank}\")\n",
    "print(f\"Result matrix rank: {result_rank}\")\n",
    "print(f\"Rank preserved? {original_rank == result_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 5: Dimension calculation\n",
    "print(\"🔢 Test Case 5: Calculating dimension of vector space\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "dim_A = dimensions(A)\n",
    "expected_dim = np.linalg.matrix_rank(A)\n",
    "\n",
    "print(f\"Matrix A (4×3):\")\n",
    "print(f\"Calculated dimension using our function: {dim_A}\")\n",
    "print(f\"Expected dimension (matrix rank): {expected_dim}\")\n",
    "print(f\"Match? {abs(dim_A - expected_dim) < 1e-10}\")\n",
    "\n",
    "# Let's also check the original 4×4 matrix\n",
    "dim_V = dimensions(V)\n",
    "expected_dim_V = np.linalg.matrix_rank(V)\n",
    "\n",
    "print(f\"\\nMatrix V (4×4):\")\n",
    "print(f\"Calculated dimension using our function: {dim_V}\")\n",
    "print(f\"Expected dimension (matrix rank): {expected_dim_V}\")\n",
    "print(f\"Match? {abs(dim_V - expected_dim_V) < 1e-10}\")\n",
    "\n",
    "print(f\"\\n💡 Insight: The dimension equals the number of linearly independent vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93704257, -0.12700832, -0.32530002,  0.        ,  0.        ],\n",
       "       [ 0.31234752,  0.72140727,  0.61807005,  0.        ,  0.        ],\n",
       "       [ 0.15617376, -0.6807646 ,  0.71566005,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 6: Wide matrix (More columns than rows)\n",
    "print(\"📏 Test Case 6: Wide matrix (3×5 - more columns than rows)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# 3 rows, 5 columns - 5 vectors in 3D space (some must be linearly dependent!)\n",
    "B = np.array([[1, 3, 2, 4, 7],\n",
    "              [2, 1, 4, 1, 9],\n",
    "              [3, 2, 1, 3, 8]], dtype=np.float_)\n",
    "\n",
    "print(\"Original matrix B (3×5):\")\n",
    "print(B)\n",
    "print(f\"Shape: {B.shape}\")\n",
    "print(\"We have 5 vectors in 3D space - some must be linearly dependent!\")\n",
    "\n",
    "result_B = gsBasis(B)\n",
    "print(\"\\nAfter Gram-Schmidt:\")\n",
    "print(np.round(result_B, 6))\n",
    "\n",
    "# Count non-zero columns\n",
    "non_zero_cols = np.sum(np.linalg.norm(result_B, axis=0) > 1e-10)\n",
    "print(f\"\\n📊 Analysis:\")\n",
    "print(f\"Number of non-zero columns: {non_zero_cols}\")\n",
    "print(f\"Expected (max rank in 3D): 3\")\n",
    "print(f\"Column norms: {[f'{la.norm(result_B[:, i]):.6f}' for i in range(B.shape[1])]}\")\n",
    "\n",
    "# Check which columns became zero (indicating linear dependence)\n",
    "zero_cols = [i for i in range(B.shape[1]) if la.norm(result_B[:, i]) < 1e-10]\n",
    "if zero_cols:\n",
    "    print(f\"Zero columns (linearly dependent): {zero_cols}\")\n",
    "else:\n",
    "    print(\"No zero columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 7: Dimension of wide matrix\n",
    "print(\"🔢 Test Case 7: Dimension of wide matrix\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "dim_B = dimensions(B)\n",
    "expected_dim_B = np.linalg.matrix_rank(B)\n",
    "\n",
    "print(f\"Matrix B (3×5):\")\n",
    "print(f\"Calculated dimension: {dim_B}\")\n",
    "print(f\"Expected dimension (matrix rank): {expected_dim_B}\")\n",
    "print(f\"Match? {abs(dim_B - expected_dim_B) < 1e-10}\")\n",
    "\n",
    "print(f\"\\n💡 Key insight: Even with 5 vectors in 3D space, the dimension is at most 3!\")\n",
    "print(f\"   The extra vectors are linear combinations of the first {int(dim_B)} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ],\n",
       "       [ 0.70710678,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 8: Explicit linear dependence\n",
    "print(\"🔗 Test Case 8: Vectors with obvious linear dependence\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a matrix where the third column is exactly the first column\n",
    "# This tests how well your algorithm detects linear dependence\n",
    "C = np.array([[1, 0, 1],    # Third column = first column\n",
    "              [0, 1, 0], \n",
    "              [2, 3, 2]], dtype=np.float_)   # Third column = first column\n",
    "\n",
    "print(\"Original matrix C:\")\n",
    "print(C)\n",
    "print(\"Notice: Column 3 = Column 1 (exact linear dependence)\")\n",
    "\n",
    "result_C = gsBasis(C)\n",
    "print(\"\\nAfter Gram-Schmidt:\")\n",
    "print(np.round(result_C, 10))\n",
    "\n",
    "print(f\"\\n📊 Analysis:\")\n",
    "print(f\"Column norms: {[f'{la.norm(result_C[:, i]):.10f}' for i in range(C.shape[1])]}\")\n",
    "\n",
    "# Check which column became zero\n",
    "zero_threshold = 1e-10\n",
    "zero_cols = [i for i in range(C.shape[1]) if la.norm(result_C[:, i]) < zero_threshold]\n",
    "print(f\"Columns that became zero: {zero_cols}\")\n",
    "\n",
    "if 2 in zero_cols:\n",
    "    print(\"✅ Correct! The linearly dependent column (column 3) became zero.\")\n",
    "else:\n",
    "    print(\"❌ Expected column 3 to become zero since it's linearly dependent.\")\n",
    "\n",
    "# Verify the remaining vectors are orthonormal\n",
    "non_zero_result = result_C[:, [i for i in range(C.shape[1]) if i not in zero_cols]]\n",
    "if non_zero_result.shape[1] > 0:\n",
    "    gram_matrix = non_zero_result.T @ non_zero_result\n",
    "    print(f\"\\nGram matrix of non-zero vectors:\")\n",
    "    print(np.round(gram_matrix, 6))\n",
    "    is_orthonormal = np.allclose(gram_matrix, np.eye(non_zero_result.shape[1]), atol=1e-10)\n",
    "    print(f\"Are remaining vectors orthonormal? {is_orthonormal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Case 9: Dimension with linear dependence\n",
    "print(\"🔢 Test Case 9: Final dimension check\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dim_C = dimensions(C)\n",
    "expected_dim_C = np.linalg.matrix_rank(C)\n",
    "\n",
    "print(f\"Matrix C (with linear dependence):\")\n",
    "print(f\"Calculated dimension: {dim_C}\")\n",
    "print(f\"Expected dimension (matrix rank): {expected_dim_C}\")\n",
    "print(f\"Match? {abs(dim_C - expected_dim_C) < 1e-10}\")\n",
    "\n",
    "print(f\"\\n🎯 Summary of all dimension tests:\")\n",
    "print(f\"Matrix V (4×4): dimension = {dimensions(V)} (rank = {np.linalg.matrix_rank(V)})\")\n",
    "print(f\"Matrix A (4×3): dimension = {dimensions(A)} (rank = {np.linalg.matrix_rank(A)})\")\n",
    "print(f\"Matrix B (3×5): dimension = {dimensions(B)} (rank = {np.linalg.matrix_rank(B)})\")\n",
    "print(f\"Matrix C (3×3): dimension = {dimensions(C)} (rank = {np.linalg.matrix_rank(C)})\")\n",
    "\n",
    "print(f\"\\n✨ If all tests pass, your Gram-Schmidt implementation is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## 🔍 Solutions and Explanations\n",
    "\n",
    "<details>\n",
    "<summary><strong>Click here to reveal the complete solutions</strong> (Try implementing it yourself first!)</summary>\n",
    "\n",
    "### Solution for `gsBasis4(A)`\n",
    "\n",
    "The key insight is to follow the Gram-Schmidt algorithm step by step:\n",
    "\n",
    "```python\n",
    "def gsBasis4(A):\n",
    "    B = np.array(A, dtype=np.float_)\n",
    "    \n",
    "    # Step 1: Normalize first vector\n",
    "    B[:, 0] = B[:, 0] / la.norm(B[:, 0])\n",
    "    \n",
    "    # Step 2: Orthogonalize and normalize second vector\n",
    "    B[:, 1] = B[:, 1] - B[:, 1] @ B[:, 0] * B[:, 0]\n",
    "    if la.norm(B[:, 1]) > verySmallNumber:\n",
    "        B[:, 1] = B[:, 1] / la.norm(B[:, 1])\n",
    "    else:\n",
    "        B[:, 1] = np.zeros_like(B[:, 1])\n",
    "    \n",
    "    # Step 3: Orthogonalize third vector against first two\n",
    "    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 0] * B[:, 0]  # Remove component parallel to u₀\n",
    "    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 1] * B[:, 1]  # Remove component parallel to u₁\n",
    "    \n",
    "    # Normalize third vector\n",
    "    if la.norm(B[:, 2]) > verySmallNumber:\n",
    "        B[:, 2] = B[:, 2] / la.norm(B[:, 2])\n",
    "    else:\n",
    "        B[:, 2] = np.zeros_like(B[:, 2])\n",
    "    \n",
    "    # Step 4: Orthogonalize fourth vector against first three\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 0] * B[:, 0]  # Remove component parallel to u₀\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 1] * B[:, 1]  # Remove component parallel to u₁\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 2] * B[:, 2]  # Remove component parallel to u₂\n",
    "    \n",
    "    # Normalize fourth vector\n",
    "    if la.norm(B[:, 3]) > verySmallNumber:\n",
    "        B[:, 3] = B[:, 3] / la.norm(B[:, 3])\n",
    "    else:\n",
    "        B[:, 3] = np.zeros_like(B[:, 3])\n",
    "    \n",
    "    return B\n",
    "```\n",
    "\n",
    "### Solution for `gsBasis(A)` (General Version)\n",
    "\n",
    "The general version uses loops to avoid repetition:\n",
    "\n",
    "```python\n",
    "def gsBasis(A):\n",
    "    B = np.array(A, dtype=np.float_)\n",
    "    \n",
    "    for i in range(B.shape[1]):\n",
    "        # Remove projections onto all previous orthonormal vectors\n",
    "        for j in range(i):\n",
    "            B[:, i] = B[:, i] - (B[:, i] @ B[:, j]) * B[:, j]\n",
    "        \n",
    "        # Normalize the vector (or set to zero if linearly dependent)\n",
    "        if la.norm(B[:, i]) > verySmallNumber:\n",
    "            B[:, i] = B[:, i] / la.norm(B[:, i])\n",
    "        else:\n",
    "            B[:, i] = np.zeros_like(B[:, i])\n",
    "    \n",
    "    return B\n",
    "```\n",
    "\n",
    "### 🧠 Key Concepts Explained\n",
    "\n",
    "**Vector Projection Formula**: \n",
    "The projection of vector $\\mathbf{a}$ onto unit vector $\\mathbf{u}$ is:\n",
    "$$\\text{proj}_{\\mathbf{u}}\\mathbf{a} = (\\mathbf{a} \\cdot \\mathbf{u})\\mathbf{u}$$\n",
    "\n",
    "**Why This Works**:\n",
    "1. **Dot product** $\\mathbf{a} \\cdot \\mathbf{u}$ gives the \"amount\" of $\\mathbf{a}$ in the direction of $\\mathbf{u}$\n",
    "2. **Multiplying by $\\mathbf{u}$** converts this scalar back to a vector in the direction of $\\mathbf{u}$\n",
    "3. **Subtracting the projection** leaves only the component perpendicular to $\\mathbf{u}$\n",
    "\n",
    "**Linear Dependence Detection**:\n",
    "If a vector becomes very small (near zero) after removing all projections, it means the vector was a linear combination of the previous vectors, so we set it to exactly zero.\n",
    "\n",
    "**Numerical Stability**:\n",
    "We use `verySmallNumber = 1e-14` as a threshold to distinguish between \"truly zero\" and \"numerically small due to floating-point errors.\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Extensions and Advanced Exercises\n",
    "\n",
    "Once you've completed the basic implementation, try these challenges to deepen your understanding:\n",
    "\n",
    "### 🎯 Challenge Problems\n",
    "\n",
    "1. **Modified Gram-Schmidt**: Research and implement the \"Modified Gram-Schmidt\" algorithm, which is more numerically stable than the classical version you just implemented.\n",
    "\n",
    "2. **QR Decomposition**: Use your Gram-Schmidt function to implement QR decomposition of a matrix, where $A = QR$ with $Q$ orthogonal and $R$ upper triangular.\n",
    "\n",
    "3. **Projection Matrix**: Given an orthonormal basis, write a function to compute the projection matrix onto the subspace spanned by those vectors.\n",
    "\n",
    "4. **Gram-Schmidt with Complex Numbers**: Extend your implementation to work with complex-valued vectors (hint: use `np.conj()` for the complex conjugate).\n",
    "\n",
    "### 🧪 Create Your Own Test Cases\n",
    "\n",
    "Try creating matrices that test edge cases:\n",
    "- Nearly linearly dependent vectors (what happens?)\n",
    "- Very large or very small numbers (numerical stability)\n",
    "- Matrices with more extreme aspect ratios\n",
    "\n",
    "### 📚 Applications to Explore\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: How does Gram-Schmidt relate to finding principal components?\n",
    "- **Least Squares Regression**: How is orthogonalization used in solving linear regression problems?\n",
    "- **Signal Processing**: How does orthogonalization help in signal separation and noise reduction?\n",
    "\n",
    "### 💡 Reflection Questions\n",
    "\n",
    "1. Why is it important to check for linear dependence during the process?\n",
    "2. What would happen if we didn't normalize the vectors?\n",
    "3. How does the order of input vectors affect the final orthonormal basis?\n",
    "4. In what situations might the classical Gram-Schmidt process be numerically unstable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Playground: Experiment with your own test cases here!\n",
    "# Try creating interesting matrices and testing your implementation\n",
    "\n",
    "# Example: Create your own test matrix\n",
    "# my_matrix = np.array([[...], [...], ...], dtype=np.float_)\n",
    "# result = gsBasis(my_matrix)\n",
    "# print(\"My test result:\")\n",
    "# print(result)\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "linear-algebra-machine-learning",
   "graded_item_id": "FNk8v",
   "launcher_item_id": "DdvVk"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
