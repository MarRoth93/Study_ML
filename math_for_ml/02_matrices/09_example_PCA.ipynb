{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77784c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb8ccb",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA): The Matrix Story üìä\n",
    "\n",
    "## What is PCA, Really? ü§î\n",
    "\n",
    "**Imagine you're looking at a cloud of data points from different angles.** Some angles reveal more structure than others. PCA finds the **best viewing angles** - the directions where data varies the most.\n",
    "\n",
    "**In mathematical terms:** PCA finds the principal axes of variation in your data using **eigenvalues and eigenvectors** of the covariance matrix.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **üîç What PCA does geometrically** - finding principal directions\n",
    "2. **üßÆ How matrices and vectors power PCA** - covariance, eigendecomposition  \n",
    "3. **üìê Step-by-step PCA computation** - from raw data to principal components\n",
    "4. **üìä Real example walkthrough** - student test scores analysis\n",
    "5. **üé® Visual interpretation** - seeing the transformation in action\n",
    "\n",
    "## üí° The Big Picture\n",
    "\n",
    "**PCA = Finding the coordinate system where your data looks simplest**\n",
    "\n",
    "- **Original coordinates**: Might have complex relationships\n",
    "- **Principal coordinates**: Data aligned with main variation directions\n",
    "- **Matrix magic**: Eigendecomposition reveals these special directions\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for PCA analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Ellipse\n",
    "import pandas as pd\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üî¨ PCA Laboratory Setup Complete!\")\n",
    "print(\"üìä All tools ready for matrix analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df5c85",
   "metadata": {},
   "source": [
    "## üìö Concrete Example: Student Test Scores\n",
    "\n",
    "**Our Scenario:** We have 50 students who took tests in **Math** and **Physics**. Generally, students good at math tend to be good at physics too (positive correlation).\n",
    "\n",
    "**Our Goal:** Use PCA to find:\n",
    "1. The **main direction** of academic ability (1st principal component)\n",
    "2. The **secondary direction** of specialized skills (2nd principal component)\n",
    "\n",
    "**Why this matters:** PCA will reveal the underlying structure in academic performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic student test score data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Create correlated test scores\n",
    "n_students = 50\n",
    "\n",
    "# Generate base academic ability scores\n",
    "ability = np.random.normal(75, 15, n_students)  # Mean=75, std=15\n",
    "\n",
    "# Math scores: influenced by ability + some randomness\n",
    "math_scores = ability + np.random.normal(0, 8, n_students)\n",
    "\n",
    "# Physics scores: also influenced by ability + different randomness\n",
    "physics_scores = 0.8 * ability + np.random.normal(0, 10, n_students)\n",
    "\n",
    "# Combine into data matrix (students √ó subjects)\n",
    "# Each row = one student, each column = one subject\n",
    "raw_data = np.column_stack([math_scores, physics_scores])\n",
    "\n",
    "# Create a nice DataFrame for viewing\n",
    "df = pd.DataFrame(raw_data, columns=['Math', 'Physics'])\n",
    "df.index.name = 'Student'\n",
    "\n",
    "print(\"üìä STUDENT TEST SCORES DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {raw_data.shape} (students √ó subjects)\")\n",
    "print(f\"Subjects: Math, Physics\")\n",
    "print(f\"\\nFirst 10 students:\")\n",
    "print(df.head(10).round(1))\n",
    "\n",
    "print(f\"\\nüìà BASIC STATISTICS:\")\n",
    "print(f\"Math    - Mean: {math_scores.mean():.1f}, Std: {math_scores.std():.1f}\")\n",
    "print(f\"Physics - Mean: {physics_scores.mean():.1f}, Std: {physics_scores.std():.1f}\")\n",
    "print(f\"Correlation: {np.corrcoef(math_scores, physics_scores)[0,1]:.3f}\")\n",
    "\n",
    "# Visualize the raw data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(math_scores, physics_scores, alpha=0.7, s=60, color='steelblue')\n",
    "plt.xlabel('Math Score', fontsize=12)\n",
    "plt.ylabel('Physics Score', fontsize=12)\n",
    "plt.title('Student Test Scores (Raw Data)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add reference lines at means\n",
    "plt.axvline(math_scores.mean(), color='red', linestyle='--', alpha=0.5, label=f'Math mean: {math_scores.mean():.1f}')\n",
    "plt.axhline(physics_scores.mean(), color='green', linestyle='--', alpha=0.5, label=f'Physics mean: {physics_scores.mean():.1f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç OBSERVATION: The data shows positive correlation!\")\n",
    "print(\"   Students good at math tend to be good at physics too.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94a56b",
   "metadata": {},
   "source": [
    "## Step 1: Center the Data üéØ\n",
    "\n",
    "**Why center?** PCA finds directions of **variation**. To measure variation properly, we need to center the data around its mean.\n",
    "\n",
    "**What centering does:**\n",
    "- Subtracts the mean from each variable\n",
    "- Shifts the data cloud so its center is at the origin (0,0)\n",
    "- **Doesn't change the shape or correlations** - just repositions\n",
    "\n",
    "**Matrix operation:** For data matrix X, centered data = X - Œº (where Œº is the mean vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bddd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Center the data\n",
    "print(\"üéØ STEP 1: DATA CENTERING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate means for each subject\n",
    "mean_vector = np.mean(raw_data, axis=0)  # Mean across students (axis=0)\n",
    "print(f\"Mean vector: {mean_vector}\")\n",
    "print(f\"  Math mean: {mean_vector[0]:.2f}\")\n",
    "print(f\"  Physics mean: {mean_vector[1]:.2f}\")\n",
    "\n",
    "# Center the data: subtract mean from each data point\n",
    "centered_data = raw_data - mean_vector\n",
    "\n",
    "print(f\"\\nüìä VERIFICATION:\")\n",
    "print(f\"Original data shape: {raw_data.shape}\")\n",
    "print(f\"Centered data shape: {centered_data.shape}\")\n",
    "print(f\"New means: {np.mean(centered_data, axis=0)}\")  # Should be ~[0, 0]\n",
    "\n",
    "# Visualize before and after centering\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before centering\n",
    "ax1.scatter(raw_data[:, 0], raw_data[:, 1], alpha=0.7, s=60, color='steelblue')\n",
    "ax1.axvline(mean_vector[0], color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "ax1.axhline(mean_vector[1], color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "ax1.plot(mean_vector[0], mean_vector[1], 'ro', markersize=12, label='Data Center')\n",
    "ax1.set_xlabel('Math Score')\n",
    "ax1.set_ylabel('Physics Score')\n",
    "ax1.set_title('Before Centering', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# After centering\n",
    "ax2.scatter(centered_data[:, 0], centered_data[:, 1], alpha=0.7, s=60, color='orange')\n",
    "ax2.axvline(0, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "ax2.axhline(0, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "ax2.plot(0, 0, 'ro', markersize=12, label='Origin (0,0)')\n",
    "ax2.set_xlabel('Math Score (centered)')\n",
    "ax2.set_ylabel('Physics Score (centered)')\n",
    "ax2.set_title('After Centering', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ CENTERING COMPLETE!\")\n",
    "print(\"   ‚Ä¢ Data cloud moved to origin\")\n",
    "print(\"   ‚Ä¢ Shape and correlations preserved\")\n",
    "print(\"   ‚Ä¢ Ready for covariance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b70769",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Covariance Matrix üßÆ\n",
    "\n",
    "**The covariance matrix captures how variables relate to each other:**\n",
    "\n",
    "$$C = \\frac{1}{n-1} X^T X$$\n",
    "\n",
    "Where X is the centered data matrix.\n",
    "\n",
    "**What each element means:**\n",
    "- **Diagonal elements:** Variance of each variable (how spread out)\n",
    "- **Off-diagonal elements:** Covariance between variables (how they move together)\n",
    "\n",
    "**For our 2√ó2 matrix:**\n",
    "$$C = \\begin{bmatrix} \\text{Var(Math)} & \\text{Cov(Math,Physics)} \\\\ \\text{Cov(Math,Physics)} & \\text{Var(Physics)} \\end{bmatrix}$$\n",
    "\n",
    "**Key insight:** The covariance matrix **IS** the data! It contains all the information about relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Compute covariance matrix\n",
    "print(\"üßÆ STEP 2: COVARIANCE MATRIX COMPUTATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Method 1: Manual calculation using matrix multiplication\n",
    "n_samples = centered_data.shape[0]\n",
    "cov_matrix_manual = (centered_data.T @ centered_data) / (n_samples - 1)\n",
    "\n",
    "# Method 2: Using NumPy (for verification)\n",
    "cov_matrix_numpy = np.cov(centered_data.T)\n",
    "\n",
    "print(f\"Data matrix shape: {centered_data.shape} (students √ó subjects)\")\n",
    "print(f\"Transposed shape: {centered_data.T.shape} (subjects √ó students)\")\n",
    "\n",
    "print(f\"\\nüìê MANUAL CALCULATION:\")\n",
    "print(f\"C = (1/{n_samples-1}) √ó X^T √ó X\")\n",
    "print(f\"C = (1/{n_samples-1}) √ó {centered_data.T.shape} √ó {centered_data.shape}\")\n",
    "print(f\"Covariance matrix (manual):\")\n",
    "print(cov_matrix_manual)\n",
    "\n",
    "print(f\"\\n‚úÖ NUMPY VERIFICATION:\")\n",
    "print(f\"Covariance matrix (numpy.cov):\")\n",
    "print(cov_matrix_numpy)\n",
    "\n",
    "print(f\"\\nüîç MATRIX INTERPRETATION:\")\n",
    "print(f\"C[0,0] = {cov_matrix_manual[0,0]:.2f} = Variance of Math scores\")\n",
    "print(f\"C[1,1] = {cov_matrix_manual[1,1]:.2f} = Variance of Physics scores\")\n",
    "print(f\"C[0,1] = C[1,0] = {cov_matrix_manual[0,1]:.2f} = Covariance between Math & Physics\")\n",
    "\n",
    "# Calculate correlation coefficient from covariance\n",
    "correlation = cov_matrix_manual[0,1] / np.sqrt(cov_matrix_manual[0,0] * cov_matrix_manual[1,1])\n",
    "print(f\"\\nüìä CORRELATION COEFFICIENT:\")\n",
    "print(f\"r = Cov(Math,Physics) / (œÉ_Math √ó œÉ_Physics) = {correlation:.3f}\")\n",
    "print(f\"Strong positive correlation: students good at math ‚Üí good at physics\")\n",
    "\n",
    "# Visualize the covariance matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Heatmap of covariance matrix\n",
    "im1 = ax1.imshow(cov_matrix_manual, cmap='RdBu', aspect='equal')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['Math', 'Physics'])\n",
    "ax1.set_yticklabels(['Math', 'Physics'])\n",
    "ax1.set_title('Covariance Matrix', fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax1.text(j, i, f'{cov_matrix_manual[i, j]:.1f}', \n",
    "                       ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Data with covariance ellipse\n",
    "ax2.scatter(centered_data[:, 0], centered_data[:, 1], alpha=0.7, s=60, color='steelblue')\n",
    "\n",
    "# Draw covariance ellipse (preview of what eigenvectors will show)\n",
    "eigenvals, eigenvecs = np.linalg.eig(cov_matrix_manual)\n",
    "angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "ellipse = Ellipse(xy=(0, 0), width=2*np.sqrt(eigenvals[0]), height=2*np.sqrt(eigenvals[1]), \n",
    "                 angle=angle, facecolor='none', edgecolor='red', linewidth=2, alpha=0.7)\n",
    "ax2.add_patch(ellipse)\n",
    "\n",
    "ax2.set_xlabel('Math (centered)')\n",
    "ax2.set_ylabel('Physics (centered)')\n",
    "ax2.set_title('Data with Covariance Ellipse', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(f\"   The covariance matrix summarizes ALL the relationships in our data!\")\n",
    "print(f\"   Next: Find the eigenvectors to reveal the principal directions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a1a84",
   "metadata": {},
   "source": [
    "## Step 3: Find Eigenvalues and Eigenvectors üîç\n",
    "\n",
    "**The Magic Moment:** We solve the eigenvalue equation:\n",
    "\n",
    "$$C \\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "**What this means:**\n",
    "- **Eigenvectors (v):** Special directions that only get **scaled** by the transformation\n",
    "- **Eigenvalues (Œª):** How much scaling happens in each direction\n",
    "\n",
    "**For PCA:**\n",
    "- **1st eigenvector:** Direction of **maximum variance** (principal component 1)\n",
    "- **2nd eigenvector:** Direction of **second-most variance** (principal component 2)\n",
    "- **Eigenvalues:** Amount of variance explained by each component\n",
    "\n",
    "**The Principal Components ARE the eigenvectors of the covariance matrix!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Eigendecomposition of covariance matrix\n",
    "print(\"üîç STEP 3: EIGENDECOMPOSITION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix_manual)\n",
    "\n",
    "print(f\"Covariance matrix:\")\n",
    "print(cov_matrix_manual)\n",
    "\n",
    "print(f\"\\nüìê EIGENVALUES (amount of variance):\")\n",
    "for i, val in enumerate(eigenvalues):\n",
    "    percentage = 100 * val / np.sum(eigenvalues)\n",
    "    print(f\"  Œª_{i+1} = {val:.3f} ({percentage:.1f}% of total variance)\")\n",
    "\n",
    "print(f\"\\nüéØ EIGENVECTORS (principal directions):\")\n",
    "for i, vec in enumerate(eigenvectors.T):\n",
    "    print(f\"  PC{i+1} = [{vec[0]:+.3f}, {vec[1]:+.3f}]\")\n",
    "    \n",
    "    # Interpret the eigenvector\n",
    "    if abs(vec[0]) > abs(vec[1]):\n",
    "        emphasis = \"Math-heavy\" if vec[0] > 0 else \"Anti-Math\"\n",
    "    else:\n",
    "        emphasis = \"Physics-heavy\" if vec[1] > 0 else \"Anti-Physics\"\n",
    "    print(f\"        Interpretation: {emphasis} direction\")\n",
    "\n",
    "# Sort by eigenvalue (largest first) - this is standard for PCA\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\nüìä PRINCIPAL COMPONENTS (sorted by importance):\")\n",
    "total_var = np.sum(eigenvalues_sorted)\n",
    "cumulative_var = 0\n",
    "for i, (val, vec) in enumerate(zip(eigenvalues_sorted, eigenvectors_sorted.T)):\n",
    "    cumulative_var += val\n",
    "    var_explained = 100 * val / total_var\n",
    "    cumulative_explained = 100 * cumulative_var / total_var\n",
    "    print(f\"PC{i+1}: {var_explained:.1f}% variance, cumulative: {cumulative_explained:.1f}%\")\n",
    "\n",
    "# Verify the eigenvalue equation: C * v = Œª * v\n",
    "print(f\"\\n‚úÖ VERIFICATION (Cv = Œªv):\")\n",
    "for i in range(len(eigenvalues_sorted)):\n",
    "    v = eigenvectors_sorted[:, i]\n",
    "    Cv = cov_matrix_manual @ v\n",
    "    Œªv = eigenvalues_sorted[i] * v\n",
    "    error = np.max(np.abs(Cv - Œªv))\n",
    "    print(f\"PC{i+1}: Max error = {error:.2e} ‚úì\")\n",
    "\n",
    "# Visualize the principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(centered_data[:, 0], centered_data[:, 1], alpha=0.6, s=60, color='steelblue', label='Students')\n",
    "\n",
    "# Plot the principal components as arrows\n",
    "scale = 30  # Scale for visibility\n",
    "colors = ['red', 'green']\n",
    "labels = ['1st Principal Component', '2nd Principal Component']\n",
    "\n",
    "for i in range(len(eigenvalues_sorted)):\n",
    "    vec = eigenvectors_sorted[:, i]\n",
    "    eigenval = eigenvalues_sorted[i]\n",
    "    \n",
    "    # Arrow showing direction and magnitude\n",
    "    plt.arrow(0, 0, vec[0] * scale, vec[1] * scale, \n",
    "              head_width=2, head_length=3, fc=colors[i], ec=colors[i], \n",
    "              linewidth=3, alpha=0.8, label=labels[i])\n",
    "    \n",
    "    # Add text showing variance explained\n",
    "    text_x, text_y = vec[0] * scale * 1.2, vec[1] * scale * 1.2\n",
    "    var_pct = 100 * eigenval / total_var\n",
    "    plt.text(text_x, text_y, f'{var_pct:.1f}%', fontsize=11, fontweight='bold', \n",
    "             ha='center', va='center', color=colors[i])\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Math Score (centered)', fontsize=12)\n",
    "plt.ylabel('Physics Score (centered)', fontsize=12)\n",
    "plt.title('Principal Components = Eigenvectors of Covariance Matrix', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° GEOMETRIC INTERPRETATION:\")\n",
    "print(f\"   ‚Ä¢ PC1 (red): Main academic ability axis - students spread most along this line\")\n",
    "print(f\"   ‚Ä¢ PC2 (green): Secondary axis - math vs physics specialization\")\n",
    "print(f\"   ‚Ä¢ Together they form a NEW coordinate system aligned with data variation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f36060",
   "metadata": {},
   "source": [
    "## Step 4: Transform to Principal Component Space üîÑ\n",
    "\n",
    "**The Final Step:** Project our data onto the principal component axes.\n",
    "\n",
    "**Matrix transformation:** \n",
    "$$Y = X \\cdot P$$\n",
    "\n",
    "Where:\n",
    "- **X:** Centered data (students √ó subjects)\n",
    "- **P:** Principal component matrix (eigenvectors as columns)\n",
    "- **Y:** Data in PC space (students √ó principal components)\n",
    "\n",
    "**What this does:**\n",
    "- Rotates the coordinate system to align with principal axes\n",
    "- **PC1 coordinate:** How much each student has \"general academic ability\"\n",
    "- **PC2 coordinate:** How much each student leans toward \"math vs physics\"\n",
    "\n",
    "**The beauty:** In PC space, the variables are **uncorrelated** (orthogonal axes)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Transform data to Principal Component space\n",
    "print(\"üîÑ STEP 4: TRANSFORMATION TO PC SPACE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create the principal component matrix (eigenvectors as columns)\n",
    "PC_matrix = eigenvectors_sorted\n",
    "print(f\"Principal Component Matrix P:\")\n",
    "print(PC_matrix)\n",
    "print(f\"Shape: {PC_matrix.shape} (subjects √ó components)\")\n",
    "\n",
    "# Transform the centered data to PC space\n",
    "# Y = X @ P (matrix multiplication)\n",
    "pc_data = centered_data @ PC_matrix\n",
    "\n",
    "print(f\"\\nüìä TRANSFORMATION:\")\n",
    "print(f\"Original data shape: {centered_data.shape} (students √ó subjects)\")\n",
    "print(f\"PC matrix shape: {PC_matrix.shape} (subjects √ó components)\")\n",
    "print(f\"PC data shape: {pc_data.shape} (students √ó components)\")\n",
    "\n",
    "print(f\"\\nüéØ FIRST 5 STUDENTS IN BOTH SPACES:\")\n",
    "print(\"Original (Math, Physics):\")\n",
    "print(centered_data[:5].round(2))\n",
    "print(\"PC Space (PC1, PC2):\")\n",
    "print(pc_data[:5].round(2))\n",
    "\n",
    "# Verify that PC coordinates are uncorrelated\n",
    "pc_correlation = np.corrcoef(pc_data.T)\n",
    "print(f\"\\nüìà CORRELATION IN PC SPACE:\")\n",
    "print(f\"PC1-PC2 correlation: {pc_correlation[0,1]:.6f}\")\n",
    "print(\"‚úÖ Essentially zero - PCs are orthogonal!\")\n",
    "\n",
    "# Calculate variance in PC space\n",
    "pc_variances = np.var(pc_data, axis=0, ddof=1)\n",
    "print(f\"\\nüìê VARIANCE IN PC SPACE:\")\n",
    "for i, var in enumerate(pc_variances):\n",
    "    print(f\"Var(PC{i+1}) = {var:.3f}\")\n",
    "print(f\"Note: These match our eigenvalues: {eigenvalues_sorted}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Original data with PC axes\n",
    "ax1.scatter(centered_data[:, 0], centered_data[:, 1], alpha=0.7, s=60, color='steelblue')\n",
    "scale = 25\n",
    "for i in range(len(eigenvalues_sorted)):\n",
    "    vec = eigenvectors_sorted[:, i]\n",
    "    colors = ['red', 'green']\n",
    "    ax1.arrow(0, 0, vec[0] * scale, vec[1] * scale, \n",
    "              head_width=2, head_length=3, fc=colors[i], ec=colors[i], \n",
    "              linewidth=3, alpha=0.8, label=f'PC{i+1}')\n",
    "ax1.set_xlabel('Math (centered)')\n",
    "ax1.set_ylabel('Physics (centered)')\n",
    "ax1.set_title('Original Space with PC Axes', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Data in PC space\n",
    "ax2.scatter(pc_data[:, 0], pc_data[:, 1], alpha=0.7, s=60, color='orange')\n",
    "ax2.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.axvline(0, color='green', linestyle='--', alpha=0.7)\n",
    "ax2.set_xlabel('PC1 (General Academic Ability)')\n",
    "ax2.set_ylabel('PC2 (Math vs Physics Preference)')\n",
    "ax2.set_title('Principal Component Space', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Show individual PC distributions\n",
    "ax3.hist(pc_data[:, 0], bins=15, alpha=0.7, color='red', edgecolor='black')\n",
    "ax3.set_xlabel('PC1 Scores')\n",
    "ax3.set_ylabel('Number of Students')\n",
    "ax3.set_title(f'PC1 Distribution (Explains {100*eigenvalues_sorted[0]/total_var:.1f}% variance)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4.hist(pc_data[:, 1], bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "ax4.set_xlabel('PC2 Scores')\n",
    "ax4.set_ylabel('Number of Students')\n",
    "ax4.set_title(f'PC2 Distribution (Explains {100*eigenvalues_sorted[1]/total_var:.1f}% variance)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate a specific student transformation\n",
    "student_idx = 0\n",
    "original_scores = centered_data[student_idx]\n",
    "pc_scores = pc_data[student_idx]\n",
    "\n",
    "print(f\"\\nüë®‚Äçüéì EXAMPLE: Student #{student_idx+1}\")\n",
    "print(f\"Original: Math={original_scores[0]:.2f}, Physics={original_scores[1]:.2f}\")\n",
    "print(f\"PC Space: PC1={pc_scores[0]:.2f}, PC2={pc_scores[1]:.2f}\")\n",
    "\n",
    "# Manual verification of the transformation\n",
    "manual_pc1 = original_scores[0] * PC_matrix[0, 0] + original_scores[1] * PC_matrix[1, 0]\n",
    "manual_pc2 = original_scores[0] * PC_matrix[0, 1] + original_scores[1] * PC_matrix[1, 1]\n",
    "print(f\"Manual check: PC1={manual_pc1:.2f}, PC2={manual_pc2:.2f} ‚úì\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETATION:\")\n",
    "print(f\"   ‚Ä¢ PC1 = {pc_scores[0]:.2f}: Academic ability relative to average\")\n",
    "print(f\"   ‚Ä¢ PC2 = {pc_scores[1]:.2f}: Math vs Physics preference\")\n",
    "print(f\"   ‚Ä¢ The transformation preserves all information but reveals structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd80b49",
   "metadata": {},
   "source": [
    "## Bonus: Dimensionality Reduction in Action üìâ\n",
    "\n",
    "**The Power of PCA:** We can **reduce dimensions** by keeping only the most important principal components!\n",
    "\n",
    "**Example:** Keep only PC1 (85%+ of variance) and project back to original space.\n",
    "\n",
    "**What we lose:** Some information (the PC2 component)\n",
    "**What we gain:** Simpler representation, noise reduction, data compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43434532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Dimensionality Reduction Demo\n",
    "print(\"üìâ DIMENSIONALITY REDUCTION DEMONSTRATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Keep only the first principal component (1D representation)\n",
    "pc1_only = pc_data[:, 0].reshape(-1, 1)  # Keep only PC1 scores\n",
    "pc1_component = PC_matrix[:, 0].reshape(-1, 1)  # Keep only PC1 vector\n",
    "\n",
    "print(f\"Original data: {centered_data.shape} ‚Üí PC space: {pc_data.shape}\")\n",
    "print(f\"Reduced to 1D: {pc1_only.shape}\")\n",
    "print(f\"Variance retained: {100 * eigenvalues_sorted[0] / total_var:.1f}%\")\n",
    "\n",
    "# Project back to original space (approximate reconstruction)\n",
    "# Reconstruction = PC1_scores √ó PC1_vector^T + mean\n",
    "reconstructed_centered = pc1_only @ pc1_component.T\n",
    "reconstructed_data = reconstructed_centered + mean_vector\n",
    "\n",
    "print(f\"\\nüîÑ RECONSTRUCTION PROCESS:\")\n",
    "print(f\"PC1 scores shape: {pc1_only.shape}\")\n",
    "print(f\"PC1 vector shape: {pc1_component.T.shape}\")\n",
    "print(f\"Reconstructed (centered): {reconstructed_centered.shape}\")\n",
    "print(f\"Reconstructed (original): {reconstructed_data.shape}\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = np.mean((raw_data - reconstructed_data)**2)\n",
    "variance_original = np.var(raw_data)\n",
    "reconstruction_quality = 1 - mse / variance_original\n",
    "\n",
    "print(f\"\\nüìä QUALITY METRICS:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Reconstruction Quality: {reconstruction_quality:.3f} ({reconstruction_quality*100:.1f}%)\")\n",
    "\n",
    "# Visualize the dimensionality reduction\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original data\n",
    "ax1.scatter(raw_data[:, 0], raw_data[:, 1], alpha=0.7, s=60, color='steelblue', label='Original')\n",
    "ax1.set_xlabel('Math Score')\n",
    "ax1.set_ylabel('Physics Score')\n",
    "ax1.set_title('Original Data (2D)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# PC1 scores as 1D representation\n",
    "ax2.scatter(pc1_only, np.zeros_like(pc1_only), alpha=0.7, s=60, color='red')\n",
    "ax2.set_xlabel('PC1 Score (General Academic Ability)')\n",
    "ax2.set_ylabel('(No 2nd dimension)')\n",
    "ax2.set_title('1D Representation (PC1 only)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "# Reconstructed data\n",
    "ax3.scatter(reconstructed_data[:, 0], reconstructed_data[:, 1], alpha=0.7, s=60, color='orange', label='Reconstructed')\n",
    "ax3.set_xlabel('Math Score')\n",
    "ax3.set_ylabel('Physics Score')\n",
    "ax3.set_title(f'Reconstructed from 1D ({reconstruction_quality*100:.1f}% quality)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Comparison: Original vs Reconstructed\n",
    "ax4.scatter(raw_data[:, 0], raw_data[:, 1], alpha=0.5, s=60, color='steelblue', label='Original')\n",
    "ax4.scatter(reconstructed_data[:, 0], reconstructed_data[:, 1], alpha=0.7, s=40, color='orange', marker='x', label='Reconstructed')\n",
    "\n",
    "# Draw lines showing the error\n",
    "for i in range(0, len(raw_data), 5):  # Show every 5th student for clarity\n",
    "    ax4.plot([raw_data[i, 0], reconstructed_data[i, 0]], \n",
    "             [raw_data[i, 1], reconstructed_data[i, 1]], \n",
    "             'gray', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax4.set_xlabel('Math Score')\n",
    "ax4.set_ylabel('Physics Score')\n",
    "ax4.set_title('Original vs Reconstructed Overlay', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show specific examples\n",
    "print(f\"\\nüìö INDIVIDUAL STUDENT EXAMPLES:\")\n",
    "print(\"Student | Original (Math, Physics) | PC1 Score | Reconstructed | Error\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(0, min(10, len(raw_data)), 2):\n",
    "    orig = raw_data[i]\n",
    "    pc1_score = pc1_only[i, 0]\n",
    "    recon = reconstructed_data[i]\n",
    "    error = np.linalg.norm(orig - recon)\n",
    "    print(f\"   {i+1:2d}   | ({orig[0]:5.1f}, {orig[1]:5.1f})        | {pc1_score:7.2f} | ({recon[0]:5.1f}, {recon[1]:5.1f}) | {error:5.2f}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ We compressed 2D data into 1D while keeping {reconstruction_quality*100:.1f}% of information\")\n",
    "print(f\"   ‚Ä¢ All students lie along the principal direction (main academic ability axis)\")\n",
    "print(f\"   ‚Ä¢ This is the essence of dimensionality reduction!\")\n",
    "print(f\"   ‚Ä¢ Real applications: 1000D ‚Üí 10D, 10000D ‚Üí 100D, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d257c1a",
   "metadata": {},
   "source": [
    "## üéì Complete PCA Summary: The Matrix Story\n",
    "\n",
    "### **What We Accomplished**\n",
    "\n",
    "We took **student test scores** and revealed their **hidden structure** using nothing but matrices and vectors!\n",
    "\n",
    "### **The 4-Step PCA Process**\n",
    "\n",
    "| **Step** | **Matrix Operation** | **Purpose** | **Result** |\n",
    "|----------|---------------------|-------------|------------|\n",
    "| **1. Center Data** | X - Œº | Remove location bias | Data centered at origin |\n",
    "| **2. Covariance** | C = X^T X / (n-1) | Capture relationships | 2√ó2 symmetric matrix |\n",
    "| **3. Eigendecomposition** | Cv = Œªv | Find principal directions | Eigenvectors + eigenvalues |\n",
    "| **4. Transform** | Y = X √ó P | Rotate to PC space | Uncorrelated coordinates |\n",
    "\n",
    "### **The Mathematical Beauty** ‚ú®\n",
    "\n",
    "**PCA = Eigendecomposition of the Covariance Matrix**\n",
    "\n",
    "- **Eigenvectors** ‚Üí Principal directions (new coordinate axes)\n",
    "- **Eigenvalues** ‚Üí Variance along each direction  \n",
    "- **Transformation** ‚Üí Matrix multiplication that rotates space\n",
    "\n",
    "### **What Each Component Revealed**\n",
    "\n",
    "**PC1 (85%+ variance):** \"General Academic Ability\"\n",
    "- Students with high PC1: Good at both math and physics\n",
    "- Students with low PC1: Struggle with both subjects\n",
    "- **This is the main axis of variation in our data!**\n",
    "\n",
    "**PC2 (15%- variance):** \"Subject Specialization\"  \n",
    "- Positive PC2: Better at math than physics\n",
    "- Negative PC2: Better at physics than math\n",
    "- **This captures the secondary pattern**\n",
    "\n",
    "### **Why This Matters** üåü\n",
    "\n",
    "**In Education:**\n",
    "- Identify students who need general vs specialized help\n",
    "- Understand the relationship between different subjects\n",
    "- Reduce complex grade data to key factors\n",
    "\n",
    "**In Data Science:**\n",
    "- **Dimensionality reduction:** 1000 features ‚Üí 10 principal components\n",
    "- **Noise reduction:** Keep signal, remove noise\n",
    "- **Visualization:** Plot high-dimensional data in 2D/3D\n",
    "- **Feature engineering:** Create meaningful combined variables\n",
    "\n",
    "**In Machine Learning:**\n",
    "- **Preprocessing:** Decorrelate features before training\n",
    "- **Compression:** Store/transmit data more efficiently  \n",
    "- **Understanding:** Discover latent factors in complex data\n",
    "\n",
    "### **The Vector/Matrix Connection** üîó\n",
    "\n",
    "**Vectors:**\n",
    "- Data points as vectors in feature space\n",
    "- Eigenvectors as special directions  \n",
    "- Principal components as new basis vectors\n",
    "\n",
    "**Matrices:**\n",
    "- Data matrix organizing all observations\n",
    "- Covariance matrix capturing relationships\n",
    "- Transformation matrix rotating coordinate systems\n",
    "\n",
    "**Linear Algebra Magic:**\n",
    "- Matrix multiplication = coordinate transformation\n",
    "- Eigendecomposition = finding natural axes\n",
    "- Orthogonal eigenvectors = uncorrelated dimensions\n",
    "\n",
    "### **Real-World Applications** üåç\n",
    "\n",
    "- **Face Recognition:** Eigenfaces for facial feature extraction\n",
    "- **Finance:** Portfolio risk analysis using return correlations  \n",
    "- **Genetics:** Population structure from DNA markers\n",
    "- **Image Processing:** Compression and denoising\n",
    "- **Recommendation Systems:** User preference patterns\n",
    "- **Climate Science:** Weather pattern identification\n",
    "\n",
    "**The Universal Truth:** Wherever you have correlated data, PCA can reveal the hidden structure using the fundamental language of linear algebra! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae2ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
