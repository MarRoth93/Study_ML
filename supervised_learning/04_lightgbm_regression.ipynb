{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfb0226",
   "metadata": {},
   "source": [
    "# Module 4 – Gradient Boosting & LightGBM Regressor\n",
    "Master gradient‑boosted tree ensembles: intuition, hyper‑parameter tuning, diagnostics, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6691ee5",
   "metadata": {},
   "source": [
    "## 1 | Learning Objectives\n",
    "By the end of this module you will be able to:\n",
    "\n",
    "1. **Describe** gradient boosting intuition and key LightGBM innovations.\n",
    "2. **Configure** core hyper‑parameters (`num_leaves`, `learning_rate`, `n_estimators`, `max_depth`, early‑stopping).\n",
    "3. **Train & validate** an `LGBMRegressor` with early stopping and cross‑validation.\n",
    "4. **Interpret** models using feature importance and SHAP value sketches.\n",
    "5. **Compare** LightGBM with Ridge/Lasso and justify model choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67f0b4",
   "metadata": {},
   "source": [
    "## 2 | Key Concepts & Analogies\n",
    "| Concept | Plain Explanation | Analogy |\n",
    "|---------|------------------|---------|\n",
    "| **Boosting** | Builds an ensemble *sequentially*, each new tree fits residual errors of the previous ensemble. | Relay race: every runner starts where the last finished, closing the gap. |\n",
    "| **Learning Rate (η)** | Scales how much each tree corrects the ensemble; small η ⇒ more trees but smoother learning. | Sipping hot coffee slowly vs gulping: safer but takes longer. |\n",
    "| **num_leaves** | Maximum leaves per tree; higher values capture complex patterns but risk overfitting. | Camera resolution: high res shows more detail *and* noise. |\n",
    "| **Leaf‑Wise Growth** | LightGBM splits the leaf with maximum gain first, growing unevenly. | Feeding the strongest plant branch first. |\n",
    "| **Histogram Binning** | Buckets continuous features for faster training & lower memory. | Rolling coins into sleeves instead of counting each coin. |\n",
    "| **Early Stopping** | Stops adding trees when validation loss stops improving. | Leaving a buffet when comfortably full, not stuffed. |\n",
    "| **Feature Importance** | Gain or split counts indicate influential variables. | Voting tally: loudest voices matter more. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – Imports & Settings\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from lightgbm import LGBMRegressor, plot_importance\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94409f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – Load Data & Split\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f4da5",
   "metadata": {},
   "source": [
    "### 3 | Baseline LightGBM with Early‑Stopping\n",
    "Set a generous `n_estimators`; early stopping cuts training once validation RMSE plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = dict(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    n_estimators=1000,          # upper bound\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = LGBMRegressor(**lgb_params)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print('Best iteration:', model.best_iteration_)\n",
    "print('Validation RMSE:', model.best_score_['valid_0']['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a28d8",
   "metadata": {},
   "source": [
    "### 4 | Hyper‑parameter Grid‑Search (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'min_child_samples': [10, 20],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "base_lgb = LGBMRegressor(\n",
    "    n_estimators=600,\n",
    "    objective='regression',\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search = GridSearchCV(base_lgb, grid, cv=3,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                      verbose=0)\n",
    "search.fit(X_train, y_train)\n",
    "print('Best params:', search.best_params_)\n",
    "print('CV RMSE:', -search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a052a4",
   "metadata": {},
   "source": [
    "### 5 | Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_importance(model, max_num_features=10)\n",
    "ax.set_title('Top 10 Feature Importances (gain)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2390f76",
   "metadata": {},
   "source": [
    "### 6 | SHAP Value Sketch *(optional – requires `shap`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install SHAP in a fresh environment\n",
    "# !pip install shap -q\n",
    "import shap\n",
    "explainer = shap.Explainer(model)  # TreeExplainer for LightGBM\n",
    "shap_values = explainer(X_val.iloc[:200])  # subsample for speed\n",
    "shap.summary_plot(shap_values, X_val.iloc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd81c0",
   "metadata": {},
   "source": [
    "## 7 | Interactive Checkpoints\n",
    "### 7.1 Quick Quiz ✅\n",
    "1. Boosting reduces **bias** predominantly.\n",
    "2. **False** – lower `learning_rate` may still overfit given enough trees.\n",
    "3. Besides `num_leaves`, `max_depth` limits tree complexity.\n",
    "4. Early stopping triggers after a user‑set number (e.g., 50) rounds without improvement.\n",
    "\n",
    "### 7.2 Coding Exercise 💻\n",
    "**Task:**\n",
    "1. Train **Model A** (`learning_rate=0.1`, `num_leaves=31`, `n_estimators=400`).\n",
    "2. Train **Model B** (`learning_rate=0.03`, `num_leaves=63`, `n_estimators=1200`, `early_stopping_rounds=80`).\n",
    "3. Record validation RMSE & training time (`%%time`).\n",
    "4. Explain which model you’d deploy and why.\n",
    "\n",
    "### 7.3 Reflection ✍️\n",
    "*When might a linear Ridge/Lasso beat LightGBM? Discuss data size, dimensionality & interpretability.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d1691",
   "metadata": {},
   "source": [
    "## 8 | Readings & Resources\n",
    "* **LightGBM Docs** – Parameters, Python API, FAQ\n",
    "* Microsoft: “LightGBM Cheatsheet”\n",
    "* Kaggle Blog: “How to Tune LightGBM”\n",
    "* Video: Stat‑Quest – Gradient Boosting & XGBoost\n",
    "* **Interpretability**: SHAP documentation – TreeSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed7941",
   "metadata": {},
   "source": [
    "## 9 | Optional Advanced Challenge 🌟\n",
    "**Custom Objective & Cross‑Validation Script**\n",
    "1. Implement a Huber-loss custom objective.\n",
    "2. Use `lightgbm.cv` with 5‑fold CV, early stopping, `n_estimators=10_000`, `learning_rate=0.01`.\n",
    "3. Plot CV RMSE vs iteration; find optimal.\n",
    "4. Compare with squared‑error objective on noisy targets.\n",
    "\n",
    "*Stretch:* Export best model to ONNX and reload for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35434dff",
   "metadata": {},
   "source": [
    "## 10 | Completion Checklist ✅\n",
    "Advance to **Module 5** when you can:\n",
    "* Tune LightGBM hyper‑parameters & justify with validation curves.\n",
    "* Employ early stopping effectively.\n",
    "* Explain feature importance & demonstrate a SHAP summary.\n",
    "* Compare LightGBM with linear models and articulate trade‑offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
